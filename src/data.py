"""
CLEAN DATA.PY - Core Technical Data Pipeline Only
================================================

âœ… CLEANED AND ORGANIZED:
1. Removed all sentiment and temporal decay processing
2. Pure technical data pipeline (OHLCV + indicators + targets)
3. Removed dividends and stock splits features
4. Moved stock_id to higher priority in column order
5. Standard directory structure with backup mechanisms
6. Enhanced timezone safety and error handling

SCOPE: Stock data + Technical indicators + Target variables + Time features
OUTPUT: data/processed/combined_dataset.csv (ready for LSTM/TFT baseline)
"""

import pandas as pd
import numpy as np
import yfinance as yf
from pathlib import Path
from typing import List, Dict, Optional, Union, Tuple, Any
import logging
from datetime import datetime, timedelta
import warnings
import sys
import traceback
import shutil
import os
from dataclasses import dataclass
import json
import time

# Technical Analysis
try:
    import ta
    TA_AVAILABLE = True
except ImportError:
    TA_AVAILABLE = False
    logging.warning("âš ï¸ 'ta' library not available. Install with: pip install ta")

# Configure logging
logger = logging.getLogger(__name__)
warnings.filterwarnings('ignore')

# Standard path constants
DATA_DIR = "data/processed"
BACKUP_DIR = "data/backups"
CACHE_DIR = "data/cache"
RAW_DIR = "data/raw"

# Main dataset file
COMBINED_DATASET = f"{DATA_DIR}/combined_dataset.csv"

@dataclass
class DatasetConfig:
    """Configuration for core dataset creation"""
    symbols: List[str]
    start_date: str
    end_date: str
    target_horizons: List[int]
    cache_enabled: bool = True
    validation_split: float = 0.2
    min_observations_per_symbol: int = 100

@dataclass
class DatasetMetrics:
    """Metrics for dataset quality assessment"""
    total_rows: int
    total_features: int
    symbols_count: int
    date_range: Tuple[str, str]
    target_coverage: Dict[str, float]
    data_quality_score: float
    missing_data_percentage: float
    feature_breakdown: Dict[str, int]

def create_backup(file_path: str) -> Optional[str]:
    """Create timestamped backup before overwriting"""
    file_path = Path(file_path)
    
    if file_path.exists():
        backup_dir = Path(BACKUP_DIR)
        backup_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_name = f"{file_path.stem}_backup_{timestamp}{file_path.suffix}"
        backup_path = backup_dir / backup_name
        
        try:
            shutil.copy2(file_path, backup_path)
            logger.info(f"ğŸ’¾ Backup created: {backup_path}")
            return str(backup_path)
        except Exception as e:
            logger.warning(f"âš ï¸ Backup failed for {file_path}: {e}")
            return None
    return None

def setup_data_directories():
    """Create all required data directories"""
    directories = [DATA_DIR, BACKUP_DIR, CACHE_DIR, RAW_DIR]
    
    for directory in directories:
        Path(directory).mkdir(parents=True, exist_ok=True)
    
    logger.info("ğŸ“ Data directories initialized")

def ensure_timezone_safe_dataframe(df: pd.DataFrame) -> pd.DataFrame:
    """
    Ensure all datetime columns in DataFrame are timezone-naive
    
    Args:
        df: DataFrame to process
        
    Returns:
        DataFrame with timezone-naive datetime columns
    """
    df = df.copy()
    
    # Handle datetime columns - FIXED: Added try/except and proper checks
    for col in df.columns:
        try:
            if hasattr(df[col], 'dtype') and hasattr(df[col].dtype, 'name'):
                if df[col].dtype.name.startswith('datetime64') or str(df[col].dtype).startswith('datetime64'):
                    df[col] = pd.to_datetime(df[col])
                    if hasattr(df[col].dt, 'tz') and df[col].dt.tz is not None:
                        logger.debug(f"ğŸ”§ Removing timezone from column {col}")
                        df[col] = df[col].dt.tz_localize(None)
        except Exception as e:
            logger.debug(f"ğŸ”§ Skipping timezone check for column {col}: {e}")
            continue
    
    # Handle datetime index
    try:
        if isinstance(df.index, pd.DatetimeIndex) and df.index.tz is not None:
            logger.debug(f"ğŸ”§ Removing timezone from index")
            df.index = df.index.tz_localize(None)
    except Exception:
        pass
    
    # Handle object columns that might contain datetime
    try:
        for col in df.select_dtypes(include=['object']).columns:
            if 'date' in col.lower():
                try:
                    temp_series = pd.to_datetime(df[col], errors='coerce')
                    if temp_series.notna().any():
                        df[col] = temp_series
                        if hasattr(df[col].dt, 'tz') and df[col].dt.tz is not None:
                            logger.debug(f"ğŸ”§ Removing timezone from object column {col}")
                            df[col] = df[col].dt.tz_localize(None)
                except Exception:
                    continue
    except Exception:
        pass
    
    return df

class StockDataCollector:
    """Collects and processes stock market data with enhanced caching and timezone safety"""
    
    def __init__(self, config: DatasetConfig):
        self.config = config
        self.cache_dir = Path(CACHE_DIR) / "stock_data"
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # Create symbol-to-id mapping (moved higher in priority)
        self.symbol_to_id = {
            symbol: f"stock_{idx:04d}" 
            for idx, symbol in enumerate(sorted(config.symbols), 1)
        }
        self.id_to_symbol = {v: k for k, v in self.symbol_to_id.items()}
        
    def collect_stock_data(self) -> pd.DataFrame:
        """Collect stock data for all symbols with robust error handling"""
        logger.info("ğŸ“ˆ Collecting stock market data...")
        
        all_data = []
        successful_symbols = []
        failed_symbols = []
        
        for symbol in self.config.symbols:
            try:
                logger.info(f"ğŸ“¥ Processing {symbol}...")
                
                # Check cache first
                cached_data = self._load_from_cache(symbol)
                if cached_data is not None:
                    data = cached_data
                    logger.info(f"ğŸ“¦ Loaded {symbol} from cache: {len(data)} rows")
                else:
                    # Fetch from Yahoo Finance
                    data = self._fetch_symbol_data(symbol)
                    if not data.empty:
                        self._save_to_cache(symbol, data)
                        logger.info(f"ğŸ“¥ Fetched {symbol} from Yahoo Finance: {len(data)} rows")
                
                if not data.empty and len(data) >= self.config.min_observations_per_symbol:
                    # Add identifiers (stock_id moved to higher priority)
                    data['stock_id'] = self.symbol_to_id[symbol]  # Higher priority
                    data['symbol'] = symbol
                    all_data.append(data)
                    successful_symbols.append(symbol)
                    logger.info(f"âœ… {symbol} ({self.symbol_to_id[symbol]}): {data.shape[0]} rows")
                else:
                    failed_symbols.append(symbol)
                    logger.warning(f"âš ï¸ {symbol}: Insufficient data ({len(data)} rows)")
                    
            except Exception as e:
                failed_symbols.append(symbol)
                logger.error(f"âŒ Error processing {symbol}: {e}")
                continue
        
        if all_data:
            combined_data = pd.concat(all_data, ignore_index=True)
            
            # Save symbol mapping for future reference
            self._save_symbol_mapping()
            
            # Apply timezone safety
            combined_data = ensure_timezone_safe_dataframe(combined_data)
            
            logger.info(f"âœ… Stock data collection complete: {combined_data.shape}")
            logger.info(f"ğŸ“Š Successful symbols: {', '.join([f'{s} ({self.symbol_to_id[s]})' for s in successful_symbols])}")
            if failed_symbols:
                logger.warning(f"âš ï¸ Failed symbols: {failed_symbols}")
            return combined_data
        else:
            logger.error("âŒ No stock data collected")
            return pd.DataFrame()
    
    def _fetch_symbol_data(self, symbol: str) -> pd.DataFrame:
        """Fetch data for a single symbol from Yahoo Finance with timezone fixes"""
        try:
            ticker = yf.Ticker(symbol)
            data = ticker.history(
                start=self.config.start_date,
                end=self.config.end_date,
                interval='1d'
            )
            
            if data.empty:
                return pd.DataFrame()
            
            # Enhanced timezone fix: Multiple approaches
            
            # Step 1: Handle timezone-aware index
            if isinstance(data.index, pd.DatetimeIndex) and data.index.tz is not None:
                logger.debug(f"ğŸ”§ Removing timezone from {symbol} index: {data.index.tz}")
                data.index = data.index.tz_localize(None)
            
            # Step 2: Reset index to make date a column
            data = data.reset_index()
            
            # Step 3: Handle Date column variations
            if 'Date' in data.columns:
                data['Date'] = pd.to_datetime(data['Date'])
                if hasattr(data['Date'].dt, 'tz') and data['Date'].dt.tz is not None:
                    logger.debug(f"ğŸ”§ Removing timezone from {symbol} Date column: {data['Date'].dt.tz}")
                    data['Date'] = data['Date'].dt.tz_localize(None)
                data = data.rename(columns={'Date': 'date'})
            
            # Step 4: Ensure lowercase column names
            data.columns = data.columns.str.lower()
            
            # Step 5: Remove unwanted columns (dividends, stock splits)
            columns_to_remove = ['dividends', 'stock splits', 'stock_splits']
            for col in columns_to_remove:
                if col in data.columns:
                    data = data.drop(columns=[col])
                    logger.debug(f"ğŸ—‘ï¸ Removed column: {col}")
            
            # Step 6: Final date column processing
            if 'date' not in data.columns:
                if len(data) > 0:
                    data['date'] = pd.date_range(self.config.start_date, periods=len(data), freq='D', tz=None)
                else:
                    data['date'] = pd.Series([], dtype='datetime64[ns]')
            
            # Step 7: Ensure date column is timezone-naive
            data['date'] = pd.to_datetime(data['date'])
            if hasattr(data['date'].dt, 'tz') and data['date'].dt.tz is not None:
                data['date'] = data['date'].dt.tz_localize(None)
            
            # Step 8: Sort by date
            data = data.sort_values('date').reset_index(drop=True)
            
            # Step 9: Final safety check
            data = ensure_timezone_safe_dataframe(data)
            
            return data
            
        except Exception as e:
            logger.error(f"âŒ Failed to fetch data for {symbol}: {e}")
            return pd.DataFrame()
    
    def _load_from_cache(self, symbol: str) -> Optional[pd.DataFrame]:
        """Load cached data for a symbol"""
        if not self.config.cache_enabled:
            return None
            
        cache_file = self.cache_dir / f"{symbol}_{self.config.start_date}_{self.config.end_date}.csv"
        
        if cache_file.exists():
            try:
                # Check if cache is recent (within 1 day)
                cache_age = time.time() - cache_file.stat().st_mtime
                if cache_age > 86400:  # 24 hours
                    logger.info(f"ğŸ“¦ Cache for {symbol} is stale, will refresh")
                    return None
                
                data = pd.read_csv(cache_file)
                data['date'] = pd.to_datetime(data['date'])
                
                # Apply timezone safety to cached data
                data = ensure_timezone_safe_dataframe(data)
                
                return data
            except Exception as e:
                logger.warning(f"âš ï¸ Cache load failed for {symbol}: {e}")
                return None
        
        return None
    
    def _save_to_cache(self, symbol: str, data: pd.DataFrame):
        """Save data to cache"""
        if not self.config.cache_enabled:
            return
            
        cache_file = self.cache_dir / f"{symbol}_{self.config.start_date}_{self.config.end_date}.csv"
        
        try:
            # Ensure timezone safety before caching
            data_to_cache = ensure_timezone_safe_dataframe(data)
            data_to_cache.to_csv(cache_file, index=False)
            logger.debug(f"ğŸ’¾ Cached {symbol} data")
        except Exception as e:
            logger.warning(f"âš ï¸ Cache save failed for {symbol}: {e}")

    def _save_symbol_mapping(self):
        """Save symbol-to-id mapping to JSON file for future reference"""
        try:
            # Create mapping data structure
            mapping = {
                "symbol_to_id": self.symbol_to_id,
                "id_to_symbol": self.id_to_symbol,
                "created_at": datetime.now().isoformat(),
                "symbols_metadata": {
                    symbol: {
                        "id": stock_id,
                        "index": idx,
                        "first_mapped": datetime.now().isoformat()
                    } for idx, (symbol, stock_id) in enumerate(self.symbol_to_id.items())
                }
            }
            
            # Ensure data directory exists
            Path(DATA_DIR).mkdir(parents=True, exist_ok=True)
            
            # Save mapping file
            mapping_file = Path(DATA_DIR) / "symbol_mapping.json"
            with open(mapping_file, 'w') as f:
                json.dump(mapping, f, indent=2)
                
            logger.info(f"ğŸ’¾ Symbol mapping saved: {mapping_file}")
            logger.info(f"   ğŸ·ï¸ Mapped {len(self.symbol_to_id)} symbols")
            
            return True
            
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to save symbol mapping: {e}")
            return False

class TechnicalIndicatorProcessor:
    """
    Processes technical indicators with core requirements:
    - OHLCV + MACD + EMA + VWAP + BB + RSI
    - Optional: Lag + ROC + Volatility + Momentum
    """
    
    @staticmethod
    def add_technical_indicators(data: pd.DataFrame) -> pd.DataFrame:
        """Add comprehensive technical indicators with timezone safety"""
        logger.info("ğŸ”§ Adding technical indicators (Core Pipeline)...")
        
        if not TA_AVAILABLE:
            logger.error("âŒ 'ta' library not available. Install with: pip install ta")
            return data
        
        data = data.copy()
        
        # Apply timezone safety at start
        data = ensure_timezone_safe_dataframe(data)
        
        # Sort by symbol and date for proper calculations
        data = data.sort_values(['symbol', 'date']).reset_index(drop=True)
        
        # Ensure numeric columns
        numeric_cols = ['open', 'high', 'low', 'close', 'volume']
        for col in numeric_cols:
            if col in data.columns:
                data[col] = pd.to_numeric(data[col], errors='coerce')
        
        # Group by symbol for proper calculations
        symbol_groups = data.groupby('symbol')
        
        try:
            # === CORE REQUIRED INDICATORS ===
            
            # 1. BASIC OHLCV FEATURES
            logger.info("   ğŸ“Š OHLCV basic features...")
            data['returns'] = symbol_groups['close'].pct_change()
            data['log_returns'] = data.groupby('symbol')['close'].pct_change().apply(lambda x: np.log(1 + x))
            
            # 2. EXPONENTIAL MOVING AVERAGES (EMA) - REQUIRED
            logger.info("   ğŸ“ˆ Exponential Moving Averages (EMA)...")
            for period in [5, 10, 20, 30, 50]:
                data[f'ema_{period}'] = symbol_groups['close'].transform(
                    lambda x: ta.trend.ema_indicator(x, window=period)
                )
            
            # 3. VWAP (Volume Weighted Average Price)
            logger.info("   ğŸ“Š Volume Weighted Average Price (VWAP)...")
            data['vwap'] = _calculate_proper_vwap(data)
            
            # 4. BOLLINGER BANDS (BB) - REQUIRED
            logger.info("   ğŸ“Š Bollinger Bands (BB)...")
            data['bb_upper'] = symbol_groups['close'].transform(
                lambda x: ta.volatility.bollinger_hband(x, window=20, window_dev=2)
            )
            data['bb_lower'] = symbol_groups['close'].transform(
                lambda x: ta.volatility.bollinger_lband(x, window=20, window_dev=2)
            )
            data['bb_middle'] = symbol_groups['close'].transform(
                lambda x: ta.volatility.bollinger_mavg(x, window=20)
            )
            
            # Safe division for BB features
            bb_middle_safe = data['bb_middle'].replace(0, np.nan)
            data['bb_width'] = (data['bb_upper'] - data['bb_lower']) / bb_middle_safe
            
            bb_range_safe = (data['bb_upper'] - data['bb_lower']).replace(0, np.nan)
            data['bb_position'] = (data['close'] - data['bb_lower']) / bb_range_safe
            
            # 5. RSI (Relative Strength Index) - REQUIRED
            logger.info("   ğŸ“Š Relative Strength Index (RSI)...")
            for period in [6, 14, 21]:
                data[f'rsi_{period}'] = symbol_groups['close'].transform(
                    lambda x: ta.momentum.rsi(x, window=period)
                )
            
            # 6. MACD (Moving Average Convergence Divergence) - REQUIRED
            logger.info("   ğŸ“Š MACD (Moving Average Convergence Divergence)...")
            data['macd_line'] = symbol_groups['close'].transform(
                lambda x: ta.trend.macd(x, window_slow=26, window_fast=12)
            )
            data['macd_signal'] = symbol_groups['close'].transform(
                lambda x: ta.trend.macd_signal(x, window_slow=26, window_fast=12, window_sign=9)
            )
            data['macd_diff'] = symbol_groups['close'].transform(
                lambda x: ta.trend.macd_diff(x, window_slow=26, window_fast=12, window_sign=9)
            )
            
            # === OPTIONAL INDICATORS ===
            
            # 7. SIMPLE MOVING AVERAGES (for comparison)
            logger.info("   ğŸ“ˆ Simple Moving Averages (SMA)...")
            for period in [5, 10, 20, 50]:
                data[f'sma_{period}'] = symbol_groups['close'].transform(
                    lambda x: ta.trend.sma_indicator(x, window=period)
                )
            
            # 8. VOLATILITY MEASURES
            logger.info("   ğŸ“Š Volatility measures...")
            data['volatility_20d'] = symbol_groups['returns'].transform(lambda x: x.rolling(window=20).std())
            
            # ATR calculation
            logger.info("   ğŸ“Š Average True Range (ATR)...")
            data['atr'] = _calculate_optimized_atr(data, window=14)
            
            # 9. MOMENTUM INDICATORS
            logger.info("   ğŸ“Š Momentum indicators...")
            
            stoch_k_values = []
            stoch_d_values = []
            williams_r_values = []
            
            for symbol in data['symbol'].unique():
                symbol_mask = data['symbol'] == symbol
                symbol_data = data[symbol_mask]
                if len(symbol_data) > 0:
                    # Stochastic K
                    stoch_k = ta.momentum.stoch(symbol_data['high'], symbol_data['low'], symbol_data['close'], window=14)
                    stoch_k_values.extend(stoch_k.values)
                    
                    # Stochastic D
                    stoch_d = ta.momentum.stoch_signal(symbol_data['high'], symbol_data['low'], symbol_data['close'], window=14)
                    stoch_d_values.extend(stoch_d.values)
                    
                    # Williams %R
                    williams_r = ta.momentum.williams_r(symbol_data['high'], symbol_data['low'], symbol_data['close'], lbp=14)
                    williams_r_values.extend(williams_r.values)
                else:
                    stoch_k_values.extend([np.nan] * len(symbol_data))
                    stoch_d_values.extend([np.nan] * len(symbol_data))
                    williams_r_values.extend([np.nan] * len(symbol_data))
            
            data['stoch_k'] = stoch_k_values
            data['stoch_d'] = stoch_d_values
            data['williams_r'] = williams_r_values
            
            # 10. RATE OF CHANGE (ROC)
            logger.info("   ğŸ“Š Rate of Change (ROC)...")
            for period in [5, 10, 20]:
                data[f'roc_{period}'] = symbol_groups['close'].transform(
                    lambda x: ta.momentum.roc(x, window=period)
                )
            
            # 11. VOLUME INDICATORS
            logger.info("   ğŸ“Š Volume indicators...")
            data['volume_sma_20'] = symbol_groups['volume'].transform(
                lambda x: x.rolling(window=20).mean()
            )
            data['volume_ratio'] = data['volume'] / (data['volume_sma_20'] + 1e-10)
            data['volume_trend'] = (
                symbol_groups['volume'].transform(lambda x: x.rolling(window=5).mean()) / 
                (symbol_groups['volume'].transform(lambda x: x.rolling(window=20).mean()) + 1e-10)
            )
            
            # 12. PRICE POSITION & ADDITIONAL FEATURES
            logger.info("   ğŸ“ Price position features...")
            price_range = (data['high'] - data['low']).replace(0, np.nan)
            data['price_position'] = (data['close'] - data['low']) / price_range
            
            # Enhanced gap calculation
            data['gap'] = data.groupby('symbol').apply(
                lambda x: (x['open'] - x['close'].shift(1)) / (x['close'].shift(1) + 1e-10)
            ).reset_index(level=0, drop=True)
            
            data['intraday_return'] = (data['close'] - data['open']) / (data['open'] + 1e-10)
            
            # 13. LAG FEATURES
            logger.info("   ğŸ“Š Lag features...")
            lag_columns = ['close', 'volume', 'returns', 'vwap', 'rsi_14']
            for col in lag_columns:
                if col in data.columns:
                    for lag in [1, 2, 3, 5, 10]:
                        try:
                            data[f'{col}_lag_{lag}'] = symbol_groups[col].transform(lambda x: x.shift(lag))
                        except Exception as e:
                            logger.warning(f"   âš ï¸ Error creating lag feature {col}_lag_{lag}: {e}")
                                    
            # === DATA CLEANING ===
            logger.info("   ğŸ§¹ Technical indicators cleaning...")
            
            # Replace infinite values
            data = data.replace([np.inf, -np.inf], np.nan)
            
            # Get technical indicator columns
            technical_cols = [col for col in data.columns if any(
                indicator in col.lower() for indicator in [
                    'ema_', 'sma_', 'vwap', 'bb_', 'rsi_', 'macd', 'atr', 'roc_', 'volume_', 'returns', 'volatility', 
                    'price_position', 'gap', 'intraday', '_lag_', 'stoch', 'williams'
                ]
            )]
            
            # Forward fill within symbol groups
            for col in technical_cols:
                try:
                    if col in data.columns:
                        data[col] = symbol_groups[col].transform(
                            lambda x: x.fillna(method='ffill').fillna(method='bfill')
                        )
                except Exception as e:
                    logger.warning(f"   âš ï¸ Error forward filling {col}: {e}")
                    continue
            
            # Final NaN cleanup
            data[technical_cols] = data[technical_cols].fillna(0)
            
            # Final timezone safety check
            data = ensure_timezone_safe_dataframe(data)
            
            logger.info(f"âœ… Technical indicators added: {len(technical_cols)} features")
            logger.info(f"   ğŸ”§ Core indicators: EMA, VWAP, BB, RSI, MACD")
            logger.info(f"   ğŸ“Š Optional indicators: SMA, Volatility, Momentum, ROC, Lags")
            
            return data
            
        except Exception as e:
            logger.error(f"âŒ Error adding technical indicators: {e}")
            import traceback
            traceback.print_exc()
            return data

class TargetVariableProcessor:
    """Processes target variables with FIXED forward-looking calculation"""
    
    @staticmethod
    def add_target_variables(data: pd.DataFrame, horizons: List[int]) -> pd.DataFrame:
        """Add target variables with FIXED forward-looking calculation"""
        logger.info("ğŸ¯ Adding target variables (FIXED Forward-Looking)...")
        
        data = data.copy()
        
        # Apply timezone safety
        data = ensure_timezone_safe_dataframe(data)
        
        # Sort by symbol and date
        data = data.sort_values(['symbol', 'date']).reset_index(drop=True)
        
        # Group by symbol for proper calculation
        symbol_groups = data.groupby('symbol')
        
        try:
            for horizon in horizons:
                logger.info(f"   ğŸ“… Calculating {horizon}-day forward returns...")
                
                # FIXED: Proper forward-looking return calculation
                # Formula: (future_price / current_price) - 1
                data[f'target_{horizon}d'] = symbol_groups['close'].transform(
                    lambda x: (x.shift(-horizon) / x - 1)
                )
                
                # Ensure target_5 exists for TFT compatibility
                if horizon == 5:
                    data['target_5'] = data['target_5d']
                    
                # Add additional target metrics for primary horizon
                if horizon == 5:
                    # Log returns version
                    data['target_5_log'] = symbol_groups['close'].transform(
                        lambda x: np.log(x.shift(-horizon) / x)
                    )
                    
                    # Binary direction (up/down)
                    data['target_5_direction'] = (data['target_5d'] > 0).astype(int)
            
            # === TARGET CLEANING ===
            logger.info("   ğŸ§¹ Target variable cleaning...")
            
            target_cols = [col for col in data.columns if col.startswith('target_')]
            
            for col in target_cols:
                # Replace infinite values
                original_count = data[col].notna().sum()
                data[col] = data[col].replace([np.inf, -np.inf], np.nan)
                cleaned_count = data[col].notna().sum()
                
                if original_count != cleaned_count:
                    logger.info(f"      {col}: Removed {original_count - cleaned_count} infinite values")
                
                # Enhanced outlier handling
                if col.endswith('d') and not col.endswith('_direction'):
                    valid_data = data[col].dropna()
                    if len(valid_data) > 0:
                        # Use IQR method for robust outlier detection
                        Q1 = valid_data.quantile(0.25)
                        Q3 = valid_data.quantile(0.75)
                        IQR = Q3 - Q1
                        lower_bound = Q1 - 1.5 * IQR
                        upper_bound = Q3 + 1.5 * IQR
                        
                        # Also use percentile caps as backup
                        q99 = valid_data.quantile(0.99)
                        q01 = valid_data.quantile(0.01)
                        
                        # Use the more conservative bounds
                        final_lower = max(lower_bound, q01)
                        final_upper = min(upper_bound, q99)
                        
                        # Apply bounds
                        data[col] = data[col].clip(lower=final_lower, upper=final_upper)
            
            # === TARGET COVERAGE VALIDATION ===
            logger.info("   ğŸ“Š Target variable validation...")
            
            for col in target_cols:
                valid_count = data[col].notna().sum()
                total_count = len(data)
                coverage = valid_count / total_count * 100
                
                if valid_count > 0:
                    mean_val = data[col].mean()
                    std_val = data[col].std()
                    min_val = data[col].min()
                    max_val = data[col].max()
                    median_val = data[col].median()
                    
                    logger.info(f"      {col}: {coverage:.1f}% coverage | "
                            f"Mean: {mean_val:.4f} | Std: {std_val:.4f} | "
                            f"Median: {median_val:.4f} | Range: [{min_val:.4f}, {max_val:.4f}]")
                else:
                    logger.warning(f"      {col}: No valid values!")
            
            # Final timezone safety check
            data = ensure_timezone_safe_dataframe(data)
            
            logger.info("âœ… Target variables added successfully (FIXED)")
            return data
            
        except Exception as e:
            logger.error(f"âŒ Error adding target variables: {e}")
            import traceback
            traceback.print_exc()
            return data

class TimeFeatureProcessor:
    """Processes time-based features for modeling with timezone safety"""
    
    @staticmethod
    def add_time_features(data: pd.DataFrame) -> pd.DataFrame:
        """Add comprehensive time-based features with timezone safety"""
        logger.info("â° Adding time features (Timezone-Safe)...")
        
        data = data.copy()
        
        # Apply timezone safety at start
        data = ensure_timezone_safe_dataframe(data)
        
        # Ensure date column is datetime and timezone-naive
        data['date'] = pd.to_datetime(data['date'])
        if hasattr(data['date'].dt, 'tz') and data['date'].dt.tz is not None:
            logger.info(f"   ğŸ”§ Removing timezone from date column: {data['date'].dt.tz}")
            data['date'] = data['date'].dt.tz_localize(None)
        
        # Basic time features
        data['year'] = data['date'].dt.year
        data['month'] = data['date'].dt.month
        data['day'] = data['date'].dt.day
        data['day_of_week'] = data['date'].dt.dayofweek
        data['quarter'] = data['date'].dt.quarter
        data['day_of_year'] = data['date'].dt.dayofyear
        
        # Enhanced week calculation with error handling
        try:
            data['week_of_year'] = data['date'].dt.isocalendar().week
        except:
            # Fallback for older pandas versions
            data['week_of_year'] = data['date'].dt.week
        
        # Market-specific time features
        data['is_month_end'] = data['date'].dt.is_month_end.astype(int)
        data['is_month_start'] = data['date'].dt.is_month_start.astype(int)
        data['is_quarter_end'] = data['date'].dt.is_quarter_end.astype(int)
        data['is_year_end'] = data['date'].dt.is_year_end.astype(int)
        
        # Enhanced market timing features
        data['is_weekday'] = (data['day_of_week'] < 5).astype(int)
        data['is_weekend'] = (data['day_of_week'] >= 5).astype(int)
        
        # Cyclical encoding for better ML performance
        data['month_sin'] = np.sin(2 * np.pi * data['month'] / 12)
        data['month_cos'] = np.cos(2 * np.pi * data['month'] / 12)
        data['day_of_week_sin'] = np.sin(2 * np.pi * data['day_of_week'] / 7)
        data['day_of_week_cos'] = np.cos(2 * np.pi * data['day_of_week'] / 7)
        data['day_of_year_sin'] = np.sin(2 * np.pi * data['day_of_year'] / 365.25)
        data['day_of_year_cos'] = np.cos(2 * np.pi * data['day_of_year'] / 365.25)
        
        # Create time_idx for each symbol (CRITICAL for TFT)
        data = data.sort_values(['symbol', 'date'])
        data['time_idx'] = data.groupby('symbol').cumcount()
        
        # Enhanced relative time features
        data['days_since_start'] = (data['date'] - data['date'].min()).dt.days
        
        # Market cycle features (approximate)
        data['trading_day_of_month'] = data.groupby([data['date'].dt.year, data['date'].dt.month]).cumcount() + 1
        
        # Final timezone safety check
        data = ensure_timezone_safe_dataframe(data)
        
        # Verify no time features have timezone
        time_features = [col for col in data.columns if any(
            time_word in col.lower() for time_word in ['year', 'month', 'day', 'week', 'quarter', 'time_idx']
        )]
        
        logger.info("âœ… Time features added (timezone-safe)")
        logger.info(f"   ğŸ“… Time features created: {len(time_features)}")
        logger.info(f"   ğŸ”§ Cyclical encoding applied")
        
        return data

# Helper functions for technical indicators

def _calculate_proper_vwap(data: pd.DataFrame) -> pd.Series:
    """Calculate VWAP properly per symbol"""
    vwap = pd.Series(index=data.index, dtype=float)
    
    for symbol in data['symbol'].unique():
        mask = data['symbol'] == symbol
        typical_price = (data.loc[mask, 'high'] + data.loc[mask, 'low'] + data.loc[mask, 'close']) / 3
        vwap[mask] = (typical_price * data.loc[mask, 'volume']).cumsum() / data.loc[mask, 'volume'].cumsum()
        
    return vwap

def _calculate_optimized_atr(data: pd.DataFrame, window: int = 14) -> pd.Series:
    """Calculate ATR (Average True Range) with optimized performance"""
    high = data['high']
    low = data['low']
    close = data['close']
    
    # Calculate True Range
    tr1 = high - low
    tr2 = abs(high - close.shift(1))
    tr3 = abs(low - close.shift(1))
    
    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
    
    # Calculate ATR by symbol
    atr = data.groupby('symbol')['close'].transform(
        lambda x: tr.loc[x.index].rolling(window=window).mean()
    )
    
    return atr

def collect_complete_dataset(config: DatasetConfig) -> pd.DataFrame:
    """
    Main function to collect complete core dataset
    
    Args:
        config: Dataset configuration
        
    Returns:
        Complete dataset with stock data, technical indicators, targets, and time features
    """
    logger.info("ğŸš€ COLLECTING COMPLETE CORE DATASET")
    logger.info("=" * 60)
    logger.info(f"ğŸ“Š Symbols: {config.symbols}")
    logger.info(f"ğŸ“… Date range: {config.start_date} to {config.end_date}")
    logger.info(f"ğŸ¯ Target horizons: {config.target_horizons}")
    logger.info("=" * 60)
    
    try:
        # Setup directories
        setup_data_directories()
        
        # Step 1: Collect stock data
        stock_collector = StockDataCollector(config)
        stock_data = stock_collector.collect_stock_data()
        
        if stock_data.empty:
            raise ValueError("No stock data collected")
        
        # Step 2: Add technical indicators
        tech_processor = TechnicalIndicatorProcessor()
        tech_data = tech_processor.add_technical_indicators(stock_data)
        
        # Step 3: Add target variables
        target_processor = TargetVariableProcessor()
        target_data = target_processor.add_target_variables(tech_data, config.target_horizons)
        
        # Step 4: Add time features
        time_processor = TimeFeatureProcessor()
        final_data = time_processor.add_time_features(target_data)
        
        # Step 5: Organize column order (stock_id moved higher)
        final_data = _organize_column_order(final_data)
        
        # Step 6: Final validation and cleanup
        final_data = _final_validation_and_cleanup(final_data)
        
        logger.info("âœ… CORE DATASET COLLECTION COMPLETE")
        logger.info(f"   ğŸ“Š Final dataset shape: {final_data.shape}")
        logger.info(f"   ğŸ¢ Symbols: {final_data['symbol'].nunique()}")
        logger.info(f"   ğŸ“… Date range: {final_data['date'].min()} to {final_data['date'].max()}")
        logger.info(f"   ğŸ¯ Target coverage: {final_data['target_5'].notna().mean():.1%}")
        
        return final_data
        
    except Exception as e:
        logger.error(f"âŒ Core dataset collection failed: {e}")
        raise

def _organize_column_order(data: pd.DataFrame) -> pd.DataFrame:
    """Organize columns in logical order with stock_id higher priority"""
    
    # Define column order categories
    identifier_cols = ['stock_id', 'symbol', 'date']  # stock_id moved first
    
    stock_cols = ['open', 'high', 'low', 'close', 'volume']
    
    basic_features = ['returns', 'log_returns', 'vwap', 'gap', 'intraday_return', 'price_position']
    
    technical_cols = []
    for col in data.columns:
        if any(pattern in col.lower() for pattern in ['ema_', 'sma_', 'bb_', 'rsi_', 'macd', 'atr', 'roc_', 'stoch', 'williams']):
            technical_cols.append(col)
    
    volume_cols = [col for col in data.columns if 'volume_' in col.lower() and col not in stock_cols]
    
    volatility_cols = [col for col in data.columns if 'volatility' in col.lower()]
    
    lag_cols = [col for col in data.columns if '_lag_' in col]
    
    time_cols = []
    for col in data.columns:
        if any(pattern in col.lower() for pattern in ['year', 'month', 'day', 'week', 'quarter', 'time_idx', '_sin', '_cos']):
            time_cols.append(col)
    
    target_cols = [col for col in data.columns if col.startswith('target_')]
    
    # Combine in logical order
    ordered_cols = (
        identifier_cols +
        stock_cols +
        basic_features +
        technical_cols +
        volume_cols +
        volatility_cols +
        lag_cols +
        time_cols +
        target_cols
    )
    
    # Add any remaining columns
    remaining_cols = [col for col in data.columns if col not in ordered_cols]
    final_order = ordered_cols + remaining_cols
    
    # Filter to existing columns only
    final_order = [col for col in final_order if col in data.columns]
    
    logger.info(f"ğŸ“‹ Column organization: {len(final_order)} columns ordered")
    logger.info(f"   ğŸ·ï¸ Identifiers: {len(identifier_cols)}")
    logger.info(f"   ğŸ“Š Stock data: {len(stock_cols)}")
    logger.info(f"   ğŸ”§ Technical: {len(technical_cols)}")
    logger.info(f"   ğŸ¯ Targets: {len(target_cols)}")
    
    return data[final_order]

def _final_validation_and_cleanup(data: pd.DataFrame) -> pd.DataFrame:
    """Final validation and cleanup of the dataset"""
    logger.info("ğŸ§¹ Final validation and cleanup...")
    
    original_shape = data.shape
    
    # Remove rows with all NaN targets
    target_cols = [col for col in data.columns if col.startswith('target_')]
    if target_cols:
        target_coverage = data[target_cols].notna().any(axis=1)
        data = data[target_coverage]
        logger.info(f"   ğŸ¯ Removed {original_shape[0] - len(data)} rows with no valid targets")
    
    # Ensure required columns exist
    required_cols = ['stock_id', 'symbol', 'date', 'open', 'high', 'low', 'close', 'volume']
    missing_cols = [col for col in required_cols if col not in data.columns]
    if missing_cols:
        raise ValueError(f"Missing required columns: {missing_cols}")
    
    # Final timezone safety
    data = ensure_timezone_safe_dataframe(data)
    
    # Summary statistics
    logger.info(f"   ğŸ“Š Final shape: {data.shape}")
    logger.info(f"   ğŸ“ˆ Data coverage: {data.notna().mean().mean():.1%}")
    logger.info(f"   ğŸ¯ Primary target coverage: {data.get('target_5', pd.Series()).notna().mean():.1%}")
    
    return data

def get_data_summary(data: pd.DataFrame) -> Dict[str, Any]:
    """Get comprehensive data summary"""
    
    summary = {
        'total_rows': len(data),
        'total_columns': len(data.columns),
        'symbols': data['symbol'].unique().tolist() if 'symbol' in data.columns else [],
        'date_range': {
            'start': str(data['date'].min()) if 'date' in data.columns else None,
            'end': str(data['date'].max()) if 'date' in data.columns else None,
            'days': (data['date'].max() - data['date'].min()).days if 'date' in data.columns else 0
        },
        'target_coverage': {},
        'feature_breakdown': {
            'stock_data': len([c for c in data.columns if c in ['open', 'high', 'low', 'close', 'volume']]),
            'technical_indicators': len([c for c in data.columns if any(p in c.lower() for p in ['ema_', 'sma_', 'rsi_', 'macd', 'bb_'])]),
            'time_features': len([c for c in data.columns if any(p in c.lower() for p in ['year', 'month', 'day', 'time_idx'])]),
            'targets': len([c for c in data.columns if c.startswith('target_')]),
            'lag_features': len([c for c in data.columns if '_lag_' in c])
        },
        'data_quality': {
            'overall_coverage': float(data.notna().mean().mean()),
            'missing_percentage': float(data.isna().mean().mean() * 100)
        }
    }
    
    # Target coverage analysis
    target_cols = [col for col in data.columns if col.startswith('target_')]
    for col in target_cols:
        summary['target_coverage'][col] = float(data[col].notna().mean())
    
    return summary

# Main execution
if __name__ == "__main__":
    # Set up logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    # Test configuration
    test_config = DatasetConfig(
        symbols=['AAPL', 'MSFT', 'GOOGL', 'NVDA', 'JPM'],
        start_date='2018-01-01',  # âœ… CORRECTED TO 2018
        end_date='2024-01-31',
        target_horizons=[5, 30, 90]
    )
    
    logger.info("ğŸš€ Testing core dataset creation...")
    logger.info("ğŸš€ CREATING CORE TECHNICAL DATASET")
    logger.info("=" * 70)
    logger.info(f"ğŸ“Š Symbols: {test_config.symbols}")
    logger.info(f"ğŸ“… Date range: {test_config.start_date} to {test_config.end_date}")
    logger.info(f"ğŸ¯ Target horizons: {test_config.target_horizons}")
    logger.info("=" * 70)
    
    try:
        # Create complete core dataset
        dataset = collect_complete_dataset(test_config)
        
        # Save to standard location
        create_backup(COMBINED_DATASET)
        dataset.to_csv(COMBINED_DATASET, index=False)
        
        # Get and save summary
        summary = get_data_summary(dataset)
        summary_path = f"{DATA_DIR}/data_summary.json"
        with open(summary_path, 'w') as f:
            json.dump(summary, f, indent=2, default=str)
        
        logger.info("âœ… CORE DATASET CREATION COMPLETE!")
        logger.info(f"ğŸ“ Saved to: {COMBINED_DATASET}")
        logger.info(f"ğŸ“Š Dataset shape: {dataset.shape}")
        logger.info(f"ğŸ¢ Symbols: {len(summary['symbols'])}")
        logger.info(f"ğŸ“… Date range: {summary['date_range']['start']} to {summary['date_range']['end']}")
        logger.info(f"ğŸ¯ Primary target coverage: {summary['target_coverage'].get('target_5', 0):.1%}")
        logger.info(f"ğŸ“ˆ Data quality: {summary['data_quality']['overall_coverage']:.1%}")
        
    except Exception as e:
        logger.error(f"âŒ Core dataset creation failed: {e}")
        if "--debug" in sys.argv:
            traceback.print_exc()