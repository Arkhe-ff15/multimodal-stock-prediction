"""
FINAL DATA.PY - Complete Implementation
=====================================

SPECIFICATIONS:
1. ‚úÖ FNSPID Data: data/raw/nasdaq_exteral_data.csv
2. ‚úÖ Exponential Decay: Standard financial Œª=0.94 (RiskMetrics standard)
3. ‚úÖ Technical Indicators: OHLCV + MACD + EMA + VWAP + BB + RSI + Optional (Lag, ROC, Volatility, Momentum)
4. ‚úÖ Three Dataset Variants:
   - Core: Stock + Technical + Targets + Time (maximum data retention)
   - Sentiment: Core + Sentiment features
   - Temporal Decay: Core + Sentiment + Exponentially decayed sentiment

DATASET ORCHESTRATION:
- LSTM/TFT Baseline ‚Üí Core Dataset (no sentiment data loss)
- TFT Sentiment ‚Üí Sentiment Dataset (core + sentiment)
- TFT Temporal Decay ‚Üí Temporal Decay Dataset (core + sentiment + decay)
"""

import pandas as pd
import numpy as np
import yfinance as yf
from pathlib import Path
from typing import List, Dict, Optional, Union, Tuple, Any
import logging
from datetime import datetime, timedelta
import warnings
import sys
import traceback
import shutil
import os
from dataclasses import dataclass
from enum import Enum
import json
import time

# Technical Analysis
try:
    import ta
    TA_AVAILABLE = True
except ImportError:
    TA_AVAILABLE = False
    logging.warning("‚ö†Ô∏è 'ta' library not available. Install with: pip install ta")

# Configure logging
logger = logging.getLogger(__name__)
warnings.filterwarnings('ignore')

# Dataset type enumeration
class DatasetType(Enum):
    CORE = "core"
    SENTIMENT = "sentiment"
    TEMPORAL_DECAY = "temporal_decay"

# Standard path constants
DATA_DIR = "data/processed"
BACKUP_DIR = "data/backups"
CACHE_DIR = "data/cache"
RAW_DIR = "data/raw"

# Dataset file paths
CORE_DATASET = f"{DATA_DIR}/core_dataset.csv"
SENTIMENT_DATASET = f"{DATA_DIR}/sentiment_dataset.csv"
TEMPORAL_DECAY_DATASET = f"{DATA_DIR}/temporal_decay_dataset.csv"
COMBINED_DATASET = f"{DATA_DIR}/combined_dataset.csv"  # Legacy compatibility

# FNSPID data location
FNSPID_DATA_FILE = f"{RAW_DIR}/nasdaq_exteral_data.csv"

@dataclass
class DatasetConfig:
    """Configuration for dataset creation"""
    symbols: List[str]
    start_date: str
    end_date: str
    target_horizons: List[int]
    fnspid_data_file: str = FNSPID_DATA_FILE
    include_sentiment: bool = True
    include_temporal_decay: bool = True
    cache_enabled: bool = True
    validation_split: float = 0.2
    min_observations_per_symbol: int = 100
    
    # Temporal decay parameters (RiskMetrics standard)
    decay_lambda: float = 0.94  # Financial industry standard
    max_decay_days: int = 90    # Maximum days to look back

@dataclass
class DatasetMetrics:
    """Metrics for dataset quality assessment"""
    total_rows: int
    total_features: int
    symbols_count: int
    date_range: Tuple[str, str]
    target_coverage: Dict[str, float]
    data_quality_score: float
    missing_data_percentage: float
    feature_breakdown: Dict[str, int]

def create_backup(file_path: str) -> Optional[str]:
    """Create timestamped backup before overwriting"""
    file_path = Path(file_path)
    
    if file_path.exists():
        backup_dir = Path(BACKUP_DIR)
        backup_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_name = f"{file_path.stem}_backup_{timestamp}{file_path.suffix}"
        backup_path = backup_dir / backup_name
        
        try:
            shutil.copy2(file_path, backup_path)
            logger.info(f"üíæ Backup created: {backup_path}")
            return str(backup_path)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Backup failed for {file_path}: {e}")
            return None
    return None

def setup_data_directories():
    """Create all required data directories"""
    directories = [DATA_DIR, BACKUP_DIR, CACHE_DIR, RAW_DIR]
    
    for directory in directories:
        Path(directory).mkdir(parents=True, exist_ok=True)
    
    logger.info("üìÅ Data directories initialized")

class StockDataCollector:
    """Collects and processes stock market data with enhanced caching"""
    
    def __init__(self, config: DatasetConfig):
        self.config = config
        self.cache_dir = Path(CACHE_DIR) / "stock_data"
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
    def collect_stock_data(self) -> pd.DataFrame:
        """Collect stock data for all symbols with robust error handling"""
        logger.info("üìà Collecting stock market data...")
        
        all_data = []
        successful_symbols = []
        failed_symbols = []
        
        for symbol in self.config.symbols:
            try:
                logger.info(f"üì• Processing {symbol}...")
                
                # Check cache first
                cached_data = self._load_from_cache(symbol)
                if cached_data is not None:
                    data = cached_data
                    logger.info(f"üì¶ Loaded {symbol} from cache: {len(data)} rows")
                else:
                    # Fetch from Yahoo Finance
                    data = self._fetch_symbol_data(symbol)
                    if not data.empty:
                        self._save_to_cache(symbol, data)
                        logger.info(f"üì• Fetched {symbol} from Yahoo Finance: {len(data)} rows")
                
                if not data.empty and len(data) >= self.config.min_observations_per_symbol:
                    # Add symbol identifier
                    data['symbol'] = symbol
                    all_data.append(data)
                    successful_symbols.append(symbol)
                    logger.info(f"‚úÖ {symbol}: {data.shape[0]} rows accepted")
                else:
                    failed_symbols.append(symbol)
                    logger.warning(f"‚ö†Ô∏è {symbol}: Insufficient data ({len(data)} rows)")
                    
            except Exception as e:
                failed_symbols.append(symbol)
                logger.error(f"‚ùå Error processing {symbol}: {e}")
                continue
        
        if all_data:
            combined_data = pd.concat(all_data, ignore_index=True)
            logger.info(f"‚úÖ Stock data collection complete: {combined_data.shape}")
            logger.info(f"üìä Successful symbols: {successful_symbols}")
            if failed_symbols:
                logger.warning(f"‚ö†Ô∏è Failed symbols: {failed_symbols}")
            return combined_data
        else:
            logger.error("‚ùå No stock data collected")
            return pd.DataFrame()
    
    def _fetch_symbol_data(self, symbol: str) -> pd.DataFrame:
        """Fetch data for a single symbol from Yahoo Finance"""
        try:
            ticker = yf.Ticker(symbol)
            data = ticker.history(
                start=self.config.start_date,
                end=self.config.end_date,
                interval='1d'
            )
            
            if data.empty:
                return pd.DataFrame()
            
            # Reset index to make date a column
            data = data.reset_index()
            data.columns = data.columns.str.lower()
            
            # Ensure date column exists
            if 'date' not in data.columns:
                data['date'] = data.index
            
            data['date'] = pd.to_datetime(data['date'])
            
            # Sort by date
            data = data.sort_values('date').reset_index(drop=True)
            
            return data
            
        except Exception as e:
            logger.error(f"‚ùå Failed to fetch data for {symbol}: {e}")
            return pd.DataFrame()
    
    def _load_from_cache(self, symbol: str) -> Optional[pd.DataFrame]:
        """Load cached data for a symbol"""
        if not self.config.cache_enabled:
            return None
            
        cache_file = self.cache_dir / f"{symbol}_{self.config.start_date}_{self.config.end_date}.csv"
        
        if cache_file.exists():
            try:
                # Check if cache is recent (within 1 day)
                cache_age = time.time() - cache_file.stat().st_mtime
                if cache_age > 86400:  # 24 hours
                    logger.info(f"üì¶ Cache for {symbol} is stale, will refresh")
                    return None
                
                data = pd.read_csv(cache_file)
                data['date'] = pd.to_datetime(data['date'])
                return data
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Cache load failed for {symbol}: {e}")
                return None
        
        return None
    
    def _save_to_cache(self, symbol: str, data: pd.DataFrame):
        """Save data to cache"""
        if not self.config.cache_enabled:
            return
            
        cache_file = self.cache_dir / f"{symbol}_{self.config.start_date}_{self.config.end_date}.csv"
        
        try:
            data.to_csv(cache_file, index=False)
            logger.debug(f"üíæ Cached {symbol} data")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Cache save failed for {symbol}: {e}")

class TechnicalIndicatorProcessor:
    """
    Processes technical indicators with YOUR SPECIFIC REQUIREMENTS:
    - OHLCV + MACD + EMA + VWAP + BB + RSI
    - Optional: Lag + ROC + Volatility + Momentum
    """
    
    @staticmethod
    def add_technical_indicators(data: pd.DataFrame) -> pd.DataFrame:
        """Add comprehensive technical indicators"""
        logger.info("üîß Adding technical indicators (OHLCV + MACD + EMA + VWAP + BB + RSI + Optional)...")
        
        if not TA_AVAILABLE:
            logger.error("‚ùå 'ta' library not available. Install with: pip install ta")
            return data
        
        data = data.copy()
        data = data.sort_values(['symbol', 'date']).reset_index(drop=True)
        
        # Ensure numeric columns
        numeric_cols = ['open', 'high', 'low', 'close', 'volume']
        for col in numeric_cols:
            if col in data.columns:
                data[col] = pd.to_numeric(data[col], errors='coerce')
        
        # Group by symbol for proper calculations
        symbol_groups = data.groupby('symbol')
        
        try:
            # === CORE REQUIRED INDICATORS ===
            
            # 1. BASIC OHLCV FEATURES
            logger.info("   üìä OHLCV basic features...")
            data['returns'] = symbol_groups['close'].pct_change()
            data['log_returns'] = data.groupby('symbol')['close'].pct_change().apply(lambda x: np.log(1 + x))            
            # 2. EXPONENTIAL MOVING AVERAGES (EMA) - REQUIRED
            logger.info("   üìà Exponential Moving Averages (EMA)...")
            for period in [5, 10, 20, 30, 50]:
                data[f'ema_{period}'] = symbol_groups['close'].transform(
                    lambda x: ta.trend.ema_indicator(x, window=period)
                )
            
            # 3. VWAP (Volume Weighted Average Price) - REQUIRED
            logger.info("   üìä Volume Weighted Average Price (VWAP)...")
            for symbol in data['symbol'].unique():
                mask = data['symbol'] == symbol
                symbol_data = data[mask]
                typical_price = (symbol_data['high'] + symbol_data['low'] + symbol_data['close']) / 3
                vwap_values = (symbol_data['volume'] * typical_price).cumsum() / symbol_data['volume'].cumsum()
                data.loc[mask, 'vwap'] = vwap_values
            
            # 4. BOLLINGER BANDS (BB) - REQUIRED
            logger.info("   üìä Bollinger Bands (BB)...")
            data['bb_upper'] = symbol_groups['close'].transform(
                lambda x: ta.volatility.bollinger_hband(x, window=20, window_dev=2)
            )
            data['bb_lower'] = symbol_groups['close'].transform(
                lambda x: ta.volatility.bollinger_lband(x, window=20, window_dev=2)
            )
            data['bb_middle'] = symbol_groups['close'].transform(
                lambda x: ta.volatility.bollinger_mavg(x, window=20)
            )
            data['bb_width'] = (data['bb_upper'] - data['bb_lower']) / data['bb_middle']
            data['bb_position'] = (data['close'] - data['bb_lower']) / (data['bb_upper'] - data['bb_lower'])
            
            # 5. RSI (Relative Strength Index) - REQUIRED
            logger.info("   üìä Relative Strength Index (RSI)...")
            for period in [6, 14, 21]:
                data[f'rsi_{period}'] = symbol_groups['close'].transform(
                    lambda x: ta.momentum.rsi(x, window=period)
                )
            
            # 6. MACD (Moving Average Convergence Divergence) - REQUIRED
            logger.info("   üìä MACD (Moving Average Convergence Divergence)...")
            data['macd_line'] = symbol_groups['close'].transform(
                lambda x: ta.trend.macd(x, window_slow=26, window_fast=12)
            )
            data['macd_signal'] = symbol_groups['close'].transform(
                lambda x: ta.trend.macd_signal(x, window_slow=26, window_fast=12, window_sign=9)
            )
            data['macd_diff'] = symbol_groups['close'].transform(
                lambda x: ta.trend.macd_diff(x, window_slow=26, window_fast=12, window_sign=9)
            )
            
            # === OPTIONAL INDICATORS ===
            
            # 7. SIMPLE MOVING AVERAGES (for comparison)
            logger.info("   üìà Simple Moving Averages (SMA)...")
            for period in [5, 10, 20, 50]:
                data[f'sma_{period}'] = symbol_groups['close'].transform(
                    lambda x: ta.trend.sma_indicator(x, window=period)
                )
            
            # 8. VOLATILITY MEASURES - OPTIONAL
            logger.info("   üìä Volatility measures...")
            data['volatility_20d'] = symbol_groups['returns'].transform(lambda x: x.rolling(window=20).std())
            data['atr'] = symbol_groups.apply(
                lambda x: ta.volatility.average_true_range(x['high'], x['low'], x['close'], window=14)
            ).reset_index(level=0, drop=True)
            
            # 9. MOMENTUM INDICATORS - OPTIONAL
            logger.info("   üìä Momentum indicators...")
            data['stoch_k'] = symbol_groups.apply(
                lambda x: ta.momentum.stoch(x['high'], x['low'], x['close'], window=14)
            ).reset_index(level=0, drop=True)
            data['stoch_d'] = symbol_groups.apply(
                lambda x: ta.momentum.stoch_signal(x['high'], x['low'], x['close'], window=14)
            ).reset_index(level=0, drop=True)
            data['williams_r'] = symbol_groups.apply(
                lambda x: ta.momentum.williams_r(x['high'], x['low'], x['close'], lbp=14)
            ).reset_index(level=0, drop=True)
            
            # 10. RATE OF CHANGE (ROC) - OPTIONAL
            logger.info("   üìä Rate of Change (ROC)...")
            for period in [5, 10, 20]:
                data[f'roc_{period}'] = symbol_groups['close'].transform(
                    lambda x: ta.momentum.roc(x, window=period)
                )
            
            # 11. VOLUME INDICATORS
            logger.info("   üìä Volume indicators...")
            data['volume_sma_20'] = symbol_groups['volume'].transform(
                lambda x: x.rolling(window=20).mean()
            )
            data['volume_ratio'] = data['volume'] / data['volume_sma_20']
            data['volume_trend'] = (
                symbol_groups['volume'].transform(lambda x: x.rolling(window=5).mean()) / 
                symbol_groups['volume'].transform(lambda x: x.rolling(window=20).mean())
            )
            
            # 12. PRICE POSITION & ADDITIONAL FEATURES
            logger.info("   üìç Price position features...")
            data['price_position'] = (data['close'] - data['low']) / (data['high'] - data['low'])
            data['gap'] = data.groupby('symbol').apply(lambda x: (x['open'] - x['close'].shift(1)) / x['close'].shift(1)).values
            data['intraday_return'] = (data['close'] - data['open']) / data['open']
            
            # 13. LAG FEATURES - OPTIONAL
            logger.info("   üìä Lag features...")
            lag_columns = ['close', 'volume', 'returns', 'vwap', 'rsi_14']
            for col in lag_columns:
                if col in data.columns:
                    for lag in [1, 2, 3, 5, 10]:
                        data[f'{col}_lag_{lag}'] = symbol_groups[col].transform(lambda x: x.shift(lag))
            
            # === DATA CLEANING ===
            logger.info("   üßπ Cleaning technical indicators...")
            
            # Replace infinite values
            data = data.replace([np.inf, -np.inf], np.nan)
            
            # Get technical indicator columns
            technical_cols = [col for col in data.columns if any(
                indicator in col.lower() for indicator in [
                    'ema_', 'sma_', 'vwap', 'bb_', 'rsi_', 'macd', 'atr', 'stoch_', 'williams_r',
                    'roc_', 'volume_', 'returns', 'volatility', 'price_position', 'gap', 'intraday',
                    '_lag_'
                ]
            )]
            
            # Forward fill within symbol groups
            for col in technical_cols:
                data[col] = symbol_groups[col].transform(
                    lambda x: x.fillna(method='ffill').fillna(method='bfill')
                )
            
            # Final NaN cleanup
            data[technical_cols] = data[technical_cols].fillna(0)
            
            logger.info(f"‚úÖ Technical indicators added: {len(technical_cols)} features")
            logger.info(f"   üîß Core indicators: EMA, VWAP, BB, RSI, MACD")
            logger.info(f"   üìä Optional indicators: SMA, Volatility, Momentum, ROC, Lags")
            
            return data
            
        except Exception as e:
            logger.error(f"‚ùå Error adding technical indicators: {e}")
            import traceback
            traceback.print_exc()
            return data

class TargetVariableProcessor:
    """Processes target variables with FIXED forward-looking calculation"""
    
    @staticmethod
    def add_target_variables(data: pd.DataFrame, horizons: List[int]) -> pd.DataFrame:
        """Add target variables with FIXED forward-looking calculation"""
        logger.info("üéØ Adding target variables (FIXED VERSION)...")
        
        data = data.copy()
        data = data.sort_values(['symbol', 'date']).reset_index(drop=True)
        
        # Group by symbol for proper calculation
        symbol_groups = data.groupby('symbol')
        
        try:
            for horizon in horizons:
                logger.info(f"   üìÖ Calculating {horizon}-day forward returns...")
                
                # FIXED: Proper forward-looking return calculation
                # Formula: (future_price / current_price) - 1
                data[f'target_{horizon}d'] = symbol_groups['close'].transform(
                    lambda x: (x.shift(-horizon) / x - 1)
                )
                
                # Ensure target_5 exists for TFT compatibility
                if horizon == 5:
                    data['target_5'] = data['target_5d']
                    
                # Add additional target metrics
                if horizon == 5:  # Add extra target variants for primary horizon
                    # Log returns version
                    data['target_5_log'] = symbol_groups['close'].transform(
                        lambda x: np.log(x.shift(-horizon) / x)
                    )
                    
                    # Binary direction (up/down)
                    data['target_5_direction'] = (data['target_5d'] > 0).astype(int)
            
            # === CLEAN TARGET VARIABLES ===
            logger.info("   üßπ Cleaning target variables...")
            
            target_cols = [col for col in data.columns if col.startswith('target_')]
            
            for col in target_cols:
                # Replace infinite values
                original_count = data[col].notna().sum()
                data[col] = data[col].replace([np.inf, -np.inf], np.nan)
                cleaned_count = data[col].notna().sum()
                
                if original_count != cleaned_count:
                    logger.info(f"      {col}: Removed {original_count - cleaned_count} infinite values")
                
                # Optional: Cap extreme values to prevent outliers
                if col.endswith('d') and not col.endswith('_direction'):  # Only for return targets
                    q99 = data[col].quantile(0.99)
                    q01 = data[col].quantile(0.01)
                    
                    # Cap at 99th/1st percentiles
                    data[col] = data[col].clip(lower=q01, upper=q99)
            
            # === VALIDATE TARGET COVERAGE ===
            logger.info("   üìä Target variable validation...")
            
            validation_results = {}
            
            for col in target_cols:
                valid_count = data[col].notna().sum()
                total_count = len(data)
                coverage = valid_count / total_count * 100
                
                if valid_count > 0:
                    mean_val = data[col].mean()
                    std_val = data[col].std()
                    min_val = data[col].min()
                    max_val = data[col].max()
                    
                    validation_results[col] = {
                        'coverage': coverage,
                        'mean': mean_val,
                        'std': std_val,
                        'range': (min_val, max_val)
                    }
                    
                    logger.info(f"      {col}: {coverage:.1f}% coverage | "
                              f"Mean: {mean_val:.4f} | Std: {std_val:.4f} | "
                              f"Range: [{min_val:.4f}, {max_val:.4f}]")
                else:
                    logger.warning(f"      {col}: No valid values!")
                    validation_results[col] = {'coverage': 0}
            
            # Check if primary target is sufficient
            if 'target_5' in validation_results:
                primary_coverage = validation_results['target_5']['coverage']
                if primary_coverage < 50:
                    logger.warning(f"‚ö†Ô∏è Primary target coverage is low: {primary_coverage:.1f}%")
                elif primary_coverage >= 80:
                    logger.info(f"‚úÖ Primary target coverage is good: {primary_coverage:.1f}%")
            
            logger.info("‚úÖ Target variables added successfully (FIXED)")
            return data
            
        except Exception as e:
            logger.error(f"‚ùå Error adding target variables: {e}")
            import traceback
            traceback.print_exc()
            return data

class TimeFeatureProcessor:
    """Processes time-based features for modeling"""
    
    @staticmethod
    def add_time_features(data: pd.DataFrame) -> pd.DataFrame:
        """Add comprehensive time-based features"""
        logger.info("‚è∞ Adding time features...")
        
        data = data.copy()
        
        # Ensure date column is datetime
        data['date'] = pd.to_datetime(data['date'])
        
        # Basic time features
        data['year'] = data['date'].dt.year
        data['month'] = data['date'].dt.month
        data['day'] = data['date'].dt.day
        data['day_of_week'] = data['date'].dt.dayofweek
        data['quarter'] = data['date'].dt.quarter
        data['day_of_year'] = data['date'].dt.dayofyear
        data['week_of_year'] = data['date'].dt.isocalendar().week
        
        # Market-specific time features
        data['is_month_end'] = data['date'].dt.is_month_end.astype(int)
        data['is_month_start'] = data['date'].dt.is_month_start.astype(int)
        data['is_quarter_end'] = data['date'].dt.is_quarter_end.astype(int)
        data['is_year_end'] = data['date'].dt.is_year_end.astype(int)
        
        # Cyclical encoding for better ML performance
        data['month_sin'] = np.sin(2 * np.pi * data['month'] / 12)
        data['month_cos'] = np.cos(2 * np.pi * data['month'] / 12)
        data['day_of_week_sin'] = np.sin(2 * np.pi * data['day_of_week'] / 7)
        data['day_of_week_cos'] = np.cos(2 * np.pi * data['day_of_week'] / 7)
        
        # Create time_idx for each symbol (CRITICAL for TFT)
        data = data.sort_values(['symbol', 'date'])
        data['time_idx'] = data.groupby('symbol').cumcount()
        
        logger.info("‚úÖ Time features added (including cyclical encoding)")
        return data

class SentimentDataProcessor:
    """Processes sentiment data from FNSPID dataset"""
    
    def __init__(self, config: DatasetConfig):
        self.config = config
        self.fnspid_file = Path(config.fnspid_data_file)
        
    def collect_sentiment_data(self) -> pd.DataFrame:
        """Collect and process sentiment data from FNSPID with comprehensive error handling"""
        logger.info("üì∞ Collecting sentiment data from FNSPID...")
        logger.info(f"üìÅ FNSPID file: {self.fnspid_file}")
        
        try:
            # Step 1: Check if FNSPID file exists
            if not self.fnspid_file.exists():
                logger.warning(f"‚ö†Ô∏è FNSPID file not found at {self.fnspid_file}")
                logger.info("üì∞ Creating empty sentiment data (file not found)")
                return self._create_empty_sentiment_data()
            
            # Step 2: Check file size and readability
            try:
                file_size_mb = self.fnspid_file.stat().st_size / (1024 * 1024)
                logger.info(f"üìä FNSPID file size: {file_size_mb:.2f} MB")
                
                if file_size_mb < 0.1:  # Less than 100KB
                    logger.warning(f"‚ö†Ô∏è FNSPID file is very small ({file_size_mb:.2f} MB), might be empty")
                    return self._create_empty_sentiment_data()
                    
            except Exception as e:
                logger.error(f"‚ùå Cannot access FNSPID file: {e}")
                return self._create_empty_sentiment_data()
            
            # Step 3: Test file readability
            try:
                logger.info("üîç Testing FNSPID file readability...")
                test_df = pd.read_csv(self.fnspid_file, nrows=3, low_memory=False)
                logger.info(f"üìã FNSPID columns detected: {list(test_df.columns)}")
                
                if len(test_df) == 0:
                    logger.warning("‚ö†Ô∏è FNSPID file appears to be empty")
                    return self._create_empty_sentiment_data()
                    
            except Exception as e:
                logger.error(f"‚ùå FNSPID file is corrupted or unreadable: {e}")
                logger.info("üì∞ Creating empty sentiment data (file unreadable)")
                return self._create_empty_sentiment_data()
            
            # Step 4: Load FNSPID data
            logger.info("üì• Loading FNSPID data...")
            fnspid_data = self._load_fnspid_data()
            
            if fnspid_data.empty:
                logger.warning("‚ö†Ô∏è FNSPID data is empty after loading")
                logger.info("üì∞ Creating empty sentiment data (no data loaded)")
                return self._create_empty_sentiment_data()
            
            # Step 5: Process sentiment data
            logger.info("üìä Processing sentiment data...")
            processed_data = self._process_sentiment_data(fnspid_data)
            
            if processed_data.empty:
                logger.warning("‚ö†Ô∏è No sentiment data after processing")
                return self._create_empty_sentiment_data()
            
            logger.info(f"‚úÖ Sentiment data collection complete: {processed_data.shape}")
            return processed_data
            
        except KeyboardInterrupt:
            logger.info("‚ö†Ô∏è Sentiment data collection interrupted by user")
            return self._create_empty_sentiment_data()
            
        except Exception as e:
            logger.error(f"‚ùå Sentiment data collection failed with unexpected error: {e}")
            logger.info("üì∞ Falling back to empty sentiment data")
            return self._create_empty_sentiment_data()
        
    def _load_fnspid_data(self) -> pd.DataFrame:
        """Load FNSPID data with smart loading strategy"""
        logger.info(f"üì• Loading FNSPID data...")
        
        try:
            # Check file size
            file_size_gb = self.fnspid_file.stat().st_size / (1024 * 1024 * 1024)
            logger.info(f"üìä FNSPID file size: {file_size_gb:.2f} GB")
            
            if file_size_gb > 2:  # Large files
                return self._load_fnspid_chunked()
            else:  # Small files
                return self._load_fnspid_direct()
                
        except Exception as e:
            logger.error(f"‚ùå Failed to load FNSPID data: {e}")
            return pd.DataFrame()
    
    def _load_fnspid_direct(self) -> pd.DataFrame:
        """Load FNSPID file directly with enhanced error handling"""
        try:
            logger.info("üìÅ Loading FNSPID file directly...")
            
            # Try different loading strategies
            loading_strategies = [
                # Strategy 1: Standard loading
                {
                    'low_memory': False,
                    'encoding': 'utf-8',
                    'on_bad_lines': 'skip',
                    'engine': 'c'
                },
                # Strategy 2: Python engine with string types
                {
                    'low_memory': False,
                    'encoding': 'utf-8',
                    'on_bad_lines': 'skip',
                    'engine': 'python',
                    'dtype': str
                },
                # Strategy 3: Basic loading
                {
                    'encoding': 'utf-8',
                    'on_bad_lines': 'skip'
                }
            ]
            
            data = None
            
            for i, strategy in enumerate(loading_strategies):
                try:
                    logger.info(f"üìä Trying loading strategy {i+1}...")
                    data = pd.read_csv(self.fnspid_file, **strategy)
                    
                    if not data.empty:
                        logger.info(f"‚úÖ Loading strategy {i+1} successful: {len(data)} rows")
                        break
                    else:
                        logger.warning(f"‚ö†Ô∏è Loading strategy {i+1} returned empty data")
                    
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Loading strategy {i+1} failed: {e}")
                    continue
            
            if data is None or data.empty:
                logger.error("‚ùå All loading strategies failed")
                return pd.DataFrame()
            
            logger.info(f"üì∞ Loaded {len(data)} total articles")
            
            # Normalize column names
            data = self._normalize_columns(data)
            
            # Filter for target symbols and date range
            filtered_data = self._filter_data(data)
            
            logger.info(f"üì∞ Filtered to {len(filtered_data)} relevant articles")
            return filtered_data
            
        except Exception as e:
            logger.error(f"‚ùå Failed to load FNSPID data directly: {e}")
            return pd.DataFrame()
    
    def _load_fnspid_chunked(self) -> pd.DataFrame:
        """Load large FNSPID files in chunks with robust error handling"""
        logger.info("üìä Loading large FNSPID file in chunks...")
        
        # Validate file first
        if not self.fnspid_file.exists():
            logger.error(f"‚ùå FNSPID file not found: {self.fnspid_file}")
            return pd.DataFrame()
        
        try:
            # Test file structure first
            logger.info("üîç Testing FNSPID file structure...")
            test_chunk = pd.read_csv(
                self.fnspid_file, 
                nrows=10,
                low_memory=False,
                encoding='utf-8',
                on_bad_lines='skip'
            )
            
            if test_chunk.empty:
                logger.warning("‚ö†Ô∏è FNSPID file appears empty in test read")
                return pd.DataFrame()
            
            logger.info(f"üìã FNSPID file structure: {test_chunk.shape}, columns: {list(test_chunk.columns)}")
            
        except Exception as e:
            logger.error(f"‚ùå FNSPID file structure test failed: {e}")
            return pd.DataFrame()
        
        # Chunked loading with enhanced error handling
        relevant_data = []
        chunk_size = 5000      # Smaller chunks for stability
        max_chunks = 50        # Limit chunks for testing
        total_processed = 0
        successful_chunks = 0
        failed_chunks = 0
        
        try:
            logger.info(f"üìä Starting chunked loading (chunk_size={chunk_size}, max_chunks={max_chunks})...")
            
            # Create chunk iterator with error handling
            try:
                chunk_iterator = pd.read_csv(
                    self.fnspid_file, 
                    chunksize=chunk_size,
                    low_memory=False,
                    encoding='utf-8',
                    on_bad_lines='skip',
                    dtype=str,  # Force string type to avoid parsing issues
                    engine='python'  # Use Python engine for better error handling
                )
            except Exception as e:
                logger.error(f"‚ùå Failed to create chunk iterator: {e}")
                return pd.DataFrame()
            
            for i, chunk in enumerate(chunk_iterator):
                # Stop if we've reached max chunks
                if i >= max_chunks:
                    logger.info(f"üìä Reached maximum chunks ({max_chunks}) - stopping for testing")
                    break
                
                try:
                    total_processed += len(chunk)
                    
                    # Normalize and filter chunk
                    chunk = self._normalize_columns(chunk)
                    
                    if chunk.empty:
                        failed_chunks += 1
                        continue
                    
                    # Filter chunk for relevant data
                    filtered_chunk = self._filter_data(chunk)
                    
                    if not filtered_chunk.empty:
                        relevant_data.append(filtered_chunk)
                        successful_chunks += 1
                        
                        if i % 10 == 0:
                            logger.info(f"   üìä Chunk {i}: {len(filtered_chunk)} relevant articles found")
                    
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Error processing chunk {i}: {e}")
                    failed_chunks += 1
                    continue
                
                # Memory management - collect garbage every 20 chunks
                if i % 20 == 0:
                    import gc
                    gc.collect()
            
            # Combine results
            if relevant_data:
                logger.info(f"üìä Combining {len(relevant_data)} successful chunks...")
                
                try:
                    combined_data = pd.concat(relevant_data, ignore_index=True)
                    
                    # Clean up memory
                    del relevant_data
                    import gc
                    gc.collect()
                    
                    logger.info(f"‚úÖ Chunked loading complete:")
                    logger.info(f"   üìä Processed {total_processed:,} total rows")
                    logger.info(f"   ‚úÖ Successful chunks: {successful_chunks}")
                    logger.info(f"   ‚ùå Failed chunks: {failed_chunks}")
                    logger.info(f"   üì∞ Relevant articles: {len(combined_data):,}")
                    
                    return combined_data
                    
                except Exception as e:
                    logger.error(f"‚ùå Failed to combine chunks: {e}")
                    return pd.DataFrame()
            else:
                logger.warning(f"‚ö†Ô∏è No relevant data found in {total_processed:,} processed rows")
                logger.info(f"   ‚úÖ Successful chunks: {successful_chunks}")
                logger.info(f"   ‚ùå Failed chunks: {failed_chunks}")
                return pd.DataFrame()
                
        except KeyboardInterrupt:
            logger.info("‚ö†Ô∏è Chunked loading interrupted by user")
            return pd.DataFrame()
            
        except Exception as e:
            logger.error(f"‚ùå Chunked loading failed: {e}")
            logger.error(f"   Error type: {type(e).__name__}")
            return pd.DataFrame()
     
    def _normalize_columns(self, data: pd.DataFrame) -> pd.DataFrame:
        """Normalize FNSPID column names with enhanced error handling"""
        try:
            if data.empty:
                return data
            
            logger.debug(f"üìã Original columns: {list(data.columns)}")
            
            # Multiple column mapping strategies
            column_mappings = [
                # Strategy 1: Exact FNSPID column names
                {
                    'Date': 'Date',
                    'Stock_symbol': 'Symbol',
                    'Article_title': 'Title',
                    'Article': 'Content',
                    'Url': 'URL'
                },
                # Strategy 2: Alternative column names
                {
                    'date': 'Date',
                    'symbol': 'Symbol',
                    'title': 'Title',
                    'content': 'Content',
                    'url': 'URL'
                },
                # Strategy 3: Lowercase variations
                {
                    'stock_symbol': 'Symbol',
                    'article_title': 'Title',
                    'article': 'Content'
                }
            ]
            
            # Apply the first matching column mapping
            for mapping in column_mappings:
                matching_cols = [col for col in mapping.keys() if col in data.columns]
                
                if matching_cols:
                    logger.info(f"üìã Applying column mapping: {matching_cols}")
                    data = data.rename(columns=mapping)
                    break
            
            logger.debug(f"üìã Normalized columns: {list(data.columns)}")
            return data
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Column normalization failed: {e}")
            return data
        
    def _filter_data(self, data: pd.DataFrame) -> pd.DataFrame:
        """Filter FNSPID data for target symbols and date range with enhanced error handling"""
        try:
            if data.empty:
                return pd.DataFrame()
            
            original_count = len(data)
            logger.info(f"üîç Filtering {original_count} articles...")
            
            # Handle date column with multiple attempts
            if 'Date' in data.columns:
                try:
                    # Try different date parsing methods
                    logger.info("üìÖ Processing date column...")
                    
                    # Method 1: Standard parsing
                    try:
                        data['Date'] = pd.to_datetime(data['Date'], errors='coerce')
                    except:
                        # Method 2: Infer format
                        try:
                            data['Date'] = pd.to_datetime(data['Date'], infer_datetime_format=True, errors='coerce')
                        except:
                            # Method 3: Force string conversion first
                            data['Date'] = pd.to_datetime(data['Date'].astype(str), errors='coerce')
                    
                    # Remove timezone if present
                    if data['Date'].dtype.name.startswith('datetime64[ns,'):
                        data['Date'] = data['Date'].dt.tz_localize(None)
                    
                    # Filter by date range
                    start_date = pd.to_datetime(self.config.start_date)
                    end_date = pd.to_datetime(self.config.end_date)
                    
                    # Count valid dates
                    valid_dates = data['Date'].notna().sum()
                    logger.info(f"üìÖ Valid dates: {valid_dates}/{len(data)}")
                    
                    if valid_dates > 0:
                        date_mask = (data['Date'] >= start_date) & (data['Date'] <= end_date)
                        data = data[date_mask]
                        logger.info(f"üìÖ Date range filter: {len(data)} articles in range")
                    
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Date filtering failed: {e}")
            
            # Filter by symbols with multiple attempts
            if 'Symbol' in data.columns:
                try:
                    logger.info("üè∑Ô∏è Processing symbol column...")
                    
                    # Clean symbol column
                    data['Symbol'] = data['Symbol'].astype(str).str.strip().str.upper()
                    
                    # Filter for target symbols
                    target_symbols = [s.upper() for s in self.config.symbols]
                    symbol_mask = data['Symbol'].isin(target_symbols)
                    data = data[symbol_mask]
                    
                    logger.info(f"üè∑Ô∏è Symbol filter: {len(data)} articles for target symbols")
                    
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Symbol filtering failed: {e}")
            
            # Remove rows with missing essential data
            if 'Content' in data.columns:
                try:
                    before_content = len(data)
                    data = data.dropna(subset=['Content'])
                    after_content = len(data)
                    
                    if before_content != after_content:
                        logger.info(f"üßπ Removed {before_content - after_content} articles with missing content")
                    
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Content filtering failed: {e}")
            
            # Final count
            final_count = len(data)
            logger.info(f"‚úÖ Filtering complete: {original_count} ‚Üí {final_count} articles")
            
            return data
            
        except Exception as e:
            logger.error(f"‚ùå Error filtering data: {e}")
            return data if isinstance(data, pd.DataFrame) else pd.DataFrame()

    
    def _process_sentiment_data(self, data: pd.DataFrame) -> pd.DataFrame:
        """Process sentiment data into time series format"""
        logger.info("üìä Processing sentiment data into time series...")
        
        try:
            # Create date range for alignment
            start_date = pd.to_datetime(self.config.start_date)
            end_date = pd.to_datetime(self.config.end_date)
            date_range = pd.date_range(start=start_date, end=end_date, freq='D')
            
            processed_data = []
            
            for symbol in self.config.symbols:
                # Filter data for this symbol
                symbol_data = data[data.get('Symbol', '') == symbol] if 'Symbol' in data.columns else data
                
                # Create time series for this symbol
                symbol_series = pd.DataFrame({
                    'date': date_range,
                    'symbol': symbol
                })
                
                # Aggregate sentiment by date
                if not symbol_data.empty and 'Date' in symbol_data.columns:
                    daily_sentiment = symbol_data.groupby(symbol_data['Date'].dt.date).agg({
                        'Title': 'count',
                        'Content': [
                            ('content_length', lambda x: len(' '.join(x.astype(str)))),
                            ('avg_content_length', lambda x: np.mean([len(str(article)) for article in x]))
                        ]
                    }).reset_index()
                    
                    # Flatten multi-level columns
                    daily_sentiment.columns = ['date', 'news_count', 'content_length', 'avg_content_length']
                    daily_sentiment['date'] = pd.to_datetime(daily_sentiment['date'])
                    
                    # Merge with symbol series
                    symbol_series = symbol_series.merge(daily_sentiment, on='date', how='left')
                
                # Fill missing values
                symbol_series['news_count'] = symbol_series.get('news_count', 0).fillna(0)
                symbol_series['content_length'] = symbol_series.get('content_length', 0).fillna(0)
                symbol_series['avg_content_length'] = symbol_series.get('avg_content_length', 0).fillna(0)
                
                # Add rolling sentiment features
                symbol_series['sentiment_momentum_3d'] = symbol_series['news_count'].rolling(window=3).mean()
                symbol_series['sentiment_momentum_7d'] = symbol_series['news_count'].rolling(window=7).mean()
                symbol_series['sentiment_momentum_14d'] = symbol_series['news_count'].rolling(window=14).mean()
                
                # Content momentum
                symbol_series['content_momentum_7d'] = symbol_series['content_length'].rolling(window=7).mean()
                
                # Sentiment intensity (news count relative to recent average)
                symbol_series['sentiment_intensity'] = (
                    symbol_series['news_count'] / 
                    (symbol_series['sentiment_momentum_14d'] + 1e-6)
                )
                
                processed_data.append(symbol_series)
            
            if processed_data:
                final_data = pd.concat(processed_data, ignore_index=True)
                
                # Fill NaN values
                sentiment_cols = [col for col in final_data.columns if col not in ['date', 'symbol']]
                final_data[sentiment_cols] = final_data[sentiment_cols].fillna(0)
                
                return final_data
            else:
                return self._create_empty_sentiment_data()
                
        except Exception as e:
            logger.error(f"‚ùå Error processing sentiment data: {e}")
            return self._create_empty_sentiment_data()
    
    def _create_empty_sentiment_data(self) -> pd.DataFrame:
        """Create empty sentiment data structure"""
        logger.info("üì∞ Creating empty sentiment data...")
        
        start_date = pd.to_datetime(self.config.start_date)
        end_date = pd.to_datetime(self.config.end_date)
        date_range = pd.date_range(start=start_date, end=end_date, freq='D')
        
        empty_data = []
        for symbol in self.config.symbols:
            symbol_data = pd.DataFrame({
                'date': date_range,
                'symbol': symbol,
                'news_count': 0,
                'content_length': 0,
                'avg_content_length': 0,
                'sentiment_momentum_3d': 0,
                'sentiment_momentum_7d': 0,
                'sentiment_momentum_14d': 0,
                'content_momentum_7d': 0,
                'sentiment_intensity': 0
            })
            empty_data.append(symbol_data)
        
        return pd.concat(empty_data, ignore_index=True)

class TemporalDecayProcessor:
    """
    Processes temporal decay features using FINANCIAL STANDARD exponential decay
    Lambda = 0.94 (RiskMetrics standard)
    """
    
    def __init__(self, config: DatasetConfig):
        self.config = config
        self.decay_lambda = config.decay_lambda  # 0.94 - RiskMetrics standard
        self.max_decay_days = config.max_decay_days  # 90 days
        
    def add_temporal_decay_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """Add exponentially decayed sentiment features using financial standard"""
        logger.info(f"‚è∞ Adding temporal decay features (Œª={self.decay_lambda})...")
        
        data = data.copy()
        data = data.sort_values(['symbol', 'date']).reset_index(drop=True)
        
        # Get sentiment columns to decay
        sentiment_cols = [col for col in data.columns if any(
            word in col.lower() for word in ['sentiment', 'news', 'content']
        ) and col not in ['date', 'symbol']]
        
        if not sentiment_cols:
            logger.warning("‚ö†Ô∏è No sentiment columns found for decay")
            return data
        
        logger.info(f"üìä Applying exponential decay to {len(sentiment_cols)} sentiment features")
        
        # Apply decay for each target horizon
        for horizon in self.config.target_horizons:
            logger.info(f"   üìÖ Processing {horizon}-day decay features...")
            
            # Calculate decay weights for this horizon
            decay_weights = self._calculate_decay_weights(horizon)
            
            # Apply decay to each sentiment column
            for col in sentiment_cols:
                new_col = f'{col}_decay_{horizon}d'
                data[new_col] = data.groupby('symbol')[col].transform(
                    lambda x: self._apply_exponential_decay(x, decay_weights)
                )
        
        # Add general decay features (not horizon-specific)
        logger.info("   üìä Adding general decay features...")
        
        for col in sentiment_cols:
            # Short-term decay (higher weight on recent data)
            data[f'{col}_decay_short'] = data.groupby('symbol')[col].transform(
                lambda x: self._apply_exponential_decay(x, self._calculate_decay_weights(5))
            )
            
            # Long-term decay (more historical data)
            data[f'{col}_decay_long'] = data.groupby('symbol')[col].transform(
                lambda x: self._apply_exponential_decay(x, self._calculate_decay_weights(30))
            )
        
        # Add decay momentum features
        logger.info("   üìä Adding decay momentum features...")
        
        decay_cols = [col for col in data.columns if '_decay_' in col]
        for col in decay_cols:
            # Momentum: current vs recent average
            data[f'{col}_momentum'] = (
                data[col] / 
                (data.groupby('symbol')[col].transform(lambda x: x.rolling(window=5).mean()) + 1e-6)
            )
        
        # Fill NaN values
        decay_cols_all = [col for col in data.columns if '_decay_' in col]
        data[decay_cols_all] = data[decay_cols_all].fillna(0)
        
        logger.info(f"‚úÖ Temporal decay features added: {len(decay_cols_all)} features")
        return data
    
    def _calculate_decay_weights(self, horizon: int) -> np.ndarray:
        """Calculate exponential decay weights for given horizon"""
        # Use RiskMetrics approach: w_t = (1-Œª) * Œª^t
        # where Œª = 0.94 (decay_lambda)
        
        # Number of days to look back (limited by max_decay_days)
        lookback_days = min(horizon * 3, self.max_decay_days)
        
        # Calculate weights
        weights = np.zeros(lookback_days)
        for t in range(lookback_days):
            weights[t] = (1 - self.decay_lambda) * (self.decay_lambda ** t)
        
        # Normalize weights to sum to 1
        weights = weights / np.sum(weights)
        
        return weights
    
    def _apply_exponential_decay(self, series: pd.Series, weights: np.ndarray) -> pd.Series:
        """Apply exponential decay to a time series"""
        result = pd.Series(index=series.index, dtype=float)
        
        for i in range(len(series)):
            # Calculate weighted average of past values
            start_idx = max(0, i - len(weights))
            end_idx = i
            
            if start_idx < end_idx:
                # Get the relevant slice of data and weights
                data_slice = series.iloc[start_idx:end_idx]
                weights_slice = weights[-(end_idx - start_idx):]
                
                # Calculate weighted average
                if len(data_slice) > 0 and len(weights_slice) > 0:
                    # Reverse data slice to align with weights (most recent first)
                    data_slice = data_slice.iloc[::-1]
                    
                    # Calculate weighted sum
                    weighted_sum = np.sum(data_slice.values * weights_slice[:len(data_slice)])
                    result.iloc[i] = weighted_sum
                else:
                    result.iloc[i] = 0
            else:
                result.iloc[i] = 0
        
        return result

class DatasetOrchestrator:
    """Orchestrates the creation of multiple dataset variants"""
    
    def __init__(self, config: DatasetConfig):
        self.config = config
        self.stock_collector = StockDataCollector(config)
        self.sentiment_processor = SentimentDataProcessor(config)
        self.temporal_processor = TemporalDecayProcessor(config)
        
        # Dataset storage
        self.core_dataset = None
        self.sentiment_dataset = None
        self.temporal_decay_dataset = None
        
    def create_all_datasets(self) -> Dict[DatasetType, pd.DataFrame]:
        """Create all dataset variants with proper orchestration"""
        logger.info("üöÄ CREATING MULTIPLE DATASET VARIANTS")
        logger.info("=" * 70)
        logger.info(f"üìä Symbols: {self.config.symbols}")
        logger.info(f"üìÖ Date range: {self.config.start_date} to {self.config.end_date}")
        logger.info(f"üéØ Target horizons: {self.config.target_horizons}")
        logger.info(f"üì∞ FNSPID file: {self.config.fnspid_data_file}")
        logger.info(f"‚è∞ Decay lambda: {self.config.decay_lambda}")
        logger.info("=" * 70)
        
        setup_data_directories()
        
        datasets = {}
        
        try:
            # Step 1: Create Core Dataset (MAXIMUM DATA RETENTION)
            logger.info("üìä STEP 1: Creating Core Dataset (Technical Indicators Only)...")
            self.core_dataset = self._create_core_dataset()
            if not self.core_dataset.empty:
                datasets[DatasetType.CORE] = self.core_dataset
                self._save_dataset(self.core_dataset, CORE_DATASET)
                logger.info("‚úÖ Core dataset created and saved")
                self._log_dataset_stats(self.core_dataset, "CORE")
            else:
                raise ValueError("‚ùå Core dataset creation failed")
            
            # Step 2: Create Sentiment Dataset (if sentiment enabled)
            if self.config.include_sentiment:
                logger.info("üì∞ STEP 2: Creating Sentiment Dataset (Core + Sentiment)...")
                self.sentiment_dataset = self._create_sentiment_dataset()
                if not self.sentiment_dataset.empty:
                    datasets[DatasetType.SENTIMENT] = self.sentiment_dataset
                    self._save_dataset(self.sentiment_dataset, SENTIMENT_DATASET)
                    logger.info("‚úÖ Sentiment dataset created and saved")
                    self._log_dataset_stats(self.sentiment_dataset, "SENTIMENT")
                else:
                    logger.warning("‚ö†Ô∏è Sentiment dataset creation failed, using core dataset")
                    datasets[DatasetType.SENTIMENT] = self.core_dataset
            
            # Step 3: Create Temporal Decay Dataset (if enabled)
            if self.config.include_temporal_decay and self.config.include_sentiment:
                logger.info("‚è∞ STEP 3: Creating Temporal Decay Dataset (Core + Sentiment + Decay)...")
                self.temporal_decay_dataset = self._create_temporal_decay_dataset()
                if not self.temporal_decay_dataset.empty:
                    datasets[DatasetType.TEMPORAL_DECAY] = self.temporal_decay_dataset
                    self._save_dataset(self.temporal_decay_dataset, TEMPORAL_DECAY_DATASET)
                    logger.info("‚úÖ Temporal decay dataset created and saved")
                    self._log_dataset_stats(self.temporal_decay_dataset, "TEMPORAL_DECAY")
                else:
                    logger.warning("‚ö†Ô∏è Temporal decay dataset creation failed, using sentiment dataset")
                    datasets[DatasetType.TEMPORAL_DECAY] = self.sentiment_dataset
            
            # Step 4: Create legacy combined dataset for backward compatibility
            logger.info("üîÑ STEP 4: Creating legacy combined dataset...")
            if self.temporal_decay_dataset is not None:
                legacy_dataset = self.temporal_decay_dataset
            elif self.sentiment_dataset is not None:
                legacy_dataset = self.sentiment_dataset
            else:
                legacy_dataset = self.core_dataset
            
            if legacy_dataset is not None:
                self._save_dataset(legacy_dataset, COMBINED_DATASET)
                logger.info("‚úÖ Legacy combined dataset saved")
            
            # Step 5: Generate dataset summaries
            self._generate_dataset_summaries(datasets)
            
            logger.info("üéâ ALL DATASETS CREATED SUCCESSFULLY!")
            self._log_final_summary(datasets)
            
            return datasets
            
        except Exception as e:
            logger.error(f"‚ùå Dataset creation failed: {e}")
            import traceback
            traceback.print_exc()
            return datasets
    
    def _create_core_dataset(self) -> pd.DataFrame:
        """Create core dataset with maximum data retention"""
        logger.info("üèóÔ∏è Building core dataset (Stock + Technical + Targets + Time)...")
        
        try:
            # Step 1: Collect stock data
            stock_data = self.stock_collector.collect_stock_data()
            
            if stock_data.empty:
                logger.error("‚ùå No stock data collected")
                return pd.DataFrame()
            
            # Step 2: Add technical indicators
            logger.info("üîß Adding technical indicators...")
            enhanced_data = TechnicalIndicatorProcessor.add_technical_indicators(stock_data)
            
            # Step 3: Add target variables
            logger.info("üéØ Adding target variables...")
            target_data = TargetVariableProcessor.add_target_variables(
                enhanced_data, self.config.target_horizons
            )
            
            # Step 4: Add time features
            logger.info("‚è∞ Adding time features...")
            final_data = TimeFeatureProcessor.add_time_features(target_data)
            
            # Step 5: Final validation
            final_data = self._validate_core_dataset(final_data)
            
            logger.info("‚úÖ Core dataset creation completed")
            return final_data
            
        except Exception as e:
            logger.error(f"‚ùå Core dataset creation failed: {e}")
            return pd.DataFrame()
    
    def _create_sentiment_dataset(self) -> pd.DataFrame:
        """Create sentiment dataset by adding sentiment features to core dataset"""
        logger.info("üèóÔ∏è Building sentiment dataset (Core + Sentiment)...")
        
        if self.core_dataset is None or self.core_dataset.empty:
            logger.error("‚ùå Core dataset required for sentiment dataset")
            return pd.DataFrame()
        
        try:
            # Start with core dataset
            sentiment_data = self.core_dataset.copy()
            
            # Collect sentiment data
            raw_sentiment = self.sentiment_processor.collect_sentiment_data()
            
            if raw_sentiment.empty:
                logger.warning("‚ö†Ô∏è No sentiment data available, using core dataset")
                return sentiment_data
            
            # Merge sentiment features
            logger.info("üîó Merging sentiment features...")
            merged_data = self._merge_sentiment_features(sentiment_data, raw_sentiment)
            
            logger.info("‚úÖ Sentiment dataset creation completed")
            return merged_data
            
        except Exception as e:
            logger.error(f"‚ùå Sentiment dataset creation failed: {e}")
            return self.core_dataset.copy() if self.core_dataset is not None else pd.DataFrame()
    
    def _create_temporal_decay_dataset(self) -> pd.DataFrame:
        """Create temporal decay dataset by adding decay features to sentiment dataset"""
        logger.info("üèóÔ∏è Building temporal decay dataset (Core + Sentiment + Decay)...")
        
        if self.sentiment_dataset is None or self.sentiment_dataset.empty:
            logger.error("‚ùå Sentiment dataset required for temporal decay dataset")
            return pd.DataFrame()
        
        try:
            # Start with sentiment dataset
            decay_data = self.sentiment_dataset.copy()
            
            # Add temporal decay features
            logger.info("‚è∞ Processing temporal decay...")
            enhanced_data = self.temporal_processor.add_temporal_decay_features(decay_data)
            
            logger.info("‚úÖ Temporal decay dataset creation completed")
            return enhanced_data
            
        except Exception as e:
            logger.error(f"‚ùå Temporal decay dataset creation failed: {e}")
            return self.sentiment_dataset.copy() if self.sentiment_dataset is not None else pd.DataFrame()
    
    def _validate_core_dataset(self, data: pd.DataFrame) -> pd.DataFrame:
        """Validate and clean core dataset"""
        logger.info("üîç Validating core dataset...")
        
        # Check target_5 coverage
        if 'target_5' in data.columns:
            target_coverage = data['target_5'].notna().mean()
            if target_coverage < 0.5:
                logger.warning(f"‚ö†Ô∏è Low target_5 coverage: {target_coverage:.1%}")
            else:
                logger.info(f"‚úÖ Target_5 coverage: {target_coverage:.1%}")
        
        # Check for sufficient data per symbol
        symbol_counts = data.groupby('symbol').size()
        insufficient_symbols = symbol_counts[symbol_counts < self.config.min_observations_per_symbol]
        
        if len(insufficient_symbols) > 0:
            logger.warning(f"‚ö†Ô∏è Symbols with insufficient data: {list(insufficient_symbols.index)}")
            # Remove symbols with insufficient data
            data = data[~data['symbol'].isin(insufficient_symbols.index)]
            logger.info(f"üßπ Removed {len(insufficient_symbols)} symbols with insufficient data")
        
        return data
    
    def _merge_sentiment_features(self, core_data: pd.DataFrame, sentiment_data: pd.DataFrame) -> pd.DataFrame:
        """Merge sentiment features with core data"""
        try:
            logger.info("üîó Merging sentiment features with core data...")
            
            # Ensure both datasets have proper date columns
            core_data = core_data.copy()
            sentiment_data = sentiment_data.copy()
            
            # Merge on date and symbol
            merged_data = core_data.merge(
                sentiment_data,
                on=['date', 'symbol'],
                how='left',
                suffixes=('', '_sentiment_dup')
            )
            
            # Remove duplicate columns
            dup_cols = [col for col in merged_data.columns if col.endswith('_sentiment_dup')]
            merged_data = merged_data.drop(columns=dup_cols)
            
            # Fill missing sentiment values with zeros
            sentiment_columns = [col for col in sentiment_data.columns 
                               if col not in ['date', 'symbol']]
            
            for col in sentiment_columns:
                if col in merged_data.columns:
                    merged_data[col] = merged_data[col].fillna(0)
            
            logger.info(f"üîó Merged {len(sentiment_columns)} sentiment features")
            return merged_data
            
        except Exception as e:
            logger.error(f"‚ùå Sentiment merge failed: {e}")
            return core_data
    
    def _save_dataset(self, data: pd.DataFrame, file_path: str):
        """Save dataset with backup"""
        try:
            # Create backup if file exists
            create_backup(file_path)
            
            # Ensure directory exists
            os.makedirs(Path(file_path).parent, exist_ok=True)
            
            # Save dataset
            if isinstance(data.index, pd.DatetimeIndex):
                data.to_csv(file_path)
            else:
                data.set_index('date').to_csv(file_path)
            
            file_size_mb = os.path.getsize(file_path) / (1024 * 1024)
            logger.info(f"üíæ Dataset saved: {file_path} ({file_size_mb:.1f} MB)")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to save dataset to {file_path}: {e}")
    
    def _log_dataset_stats(self, data: pd.DataFrame, dataset_name: str):
        """Log dataset statistics"""
        try:
            target_cols = [col for col in data.columns if col.startswith('target_')]
            sentiment_cols = [col for col in data.columns if any(
                word in col.lower() for word in ['sentiment', 'news', 'content']
            )]
            technical_cols = [col for col in data.columns if any(
                word in col.lower() for word in ['ema_', 'sma_', 'rsi_', 'macd', 'bb_', 'vwap']
            )]
            
            logger.info(f"üìä {dataset_name} DATASET STATS:")
            logger.info(f"   Shape: {data.shape}")
            logger.info(f"   Symbols: {data['symbol'].nunique()}")
            logger.info(f"   Date range: {data['date'].min().date()} to {data['date'].max().date()}")
            logger.info(f"   Technical features: {len(technical_cols)}")
            logger.info(f"   Sentiment features: {len(sentiment_cols)}")
            logger.info(f"   Target features: {len(target_cols)}")
            
            if 'target_5' in data.columns:
                coverage = data['target_5'].notna().mean()
                logger.info(f"   Target_5 coverage: {coverage:.1%}")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error logging stats for {dataset_name}: {e}")
    
    def _generate_dataset_summaries(self, datasets: Dict[DatasetType, pd.DataFrame]):
        """Generate and save dataset summaries"""
        logger.info("üìã Generating dataset summaries...")
        
        summaries = {}
        
        for dataset_type, data in datasets.items():
            metrics = self._calculate_dataset_metrics(data, dataset_type)
            
            summary = {
                'dataset_type': dataset_type.value,
                'creation_time': datetime.now().isoformat(),
                'config': {
                    'symbols': self.config.symbols,
                    'start_date': self.config.start_date,
                    'end_date': self.config.end_date,
                    'target_horizons': self.config.target_horizons,
                    'decay_lambda': self.config.decay_lambda,
                    'fnspid_file': str(self.config.fnspid_data_file)
                },
                'metrics': {
                    'total_rows': metrics.total_rows,
                    'total_features': metrics.total_features,
                    'symbols_count': metrics.symbols_count,
                    'date_range': metrics.date_range,
                    'target_coverage': metrics.target_coverage,
                    'data_quality_score': metrics.data_quality_score,
                    'missing_data_percentage': metrics.missing_data_percentage,
                    'feature_breakdown': metrics.feature_breakdown
                }
            }
            
            summaries[dataset_type.value] = summary
        
        # Save summaries to JSON
        summary_file = f"{DATA_DIR}/dataset_summaries.json"
        try:
            with open(summary_file, 'w') as f:
                json.dump(summaries, f, indent=2)
            logger.info(f"üìã Dataset summaries saved: {summary_file}")
        except Exception as e:
            logger.error(f"‚ùå Failed to save summaries: {e}")
    
    def _calculate_dataset_metrics(self, data: pd.DataFrame, dataset_type: DatasetType) -> DatasetMetrics:
        """Calculate comprehensive metrics for a dataset"""
        try:
            # Basic metrics
            total_rows = len(data)
            total_features = len(data.columns)
            symbols_count = data['symbol'].nunique() if 'symbol' in data.columns else 0
            
            # Date range
            if 'date' in data.columns:
                date_min = str(data['date'].min().date())
                date_max = str(data['date'].max().date())
                date_range = (date_min, date_max)
            else:
                date_range = ("N/A", "N/A")
            
            # Target coverage
            target_coverage = {}
            target_cols = [col for col in data.columns if col.startswith('target_')]
            for col in target_cols:
                coverage = data[col].notna().mean()
                target_coverage[col] = round(coverage * 100, 2)
            
            # Missing data percentage
            missing_percentage = (data.isnull().sum().sum() / (total_rows * total_features)) * 100
            
            # Feature breakdown
            feature_breakdown = {
                'stock': len([col for col in data.columns if col in ['open', 'high', 'low', 'close', 'volume']]),
                'technical': len([col for col in data.columns if any(
                    indicator in col.lower() for indicator in ['ema_', 'sma_', 'rsi', 'macd', 'bb_', 'vwap', 'atr', 'roc_']
                )]),
                'sentiment': len([col for col in data.columns if any(
                    word in col.lower() for word in ['sentiment', 'news', 'content']
                ) and '_decay_' not in col]),
                'temporal_decay': len([col for col in data.columns if '_decay_' in col.lower()]),
                'targets': len(target_cols),
                'time': len([col for col in data.columns if col in ['year', 'month', 'day_of_week', 'quarter', 'time_idx', 'month_sin', 'month_cos']]),
                'other': 0
            }
            
            # Calculate 'other' features
            accounted_features = sum(feature_breakdown.values())
            feature_breakdown['other'] = max(0, total_features - accounted_features)
            
            # Data quality score (0-100)
            quality_factors = [
                min(100, (1 - missing_percentage / 100) * 100),  # Less missing = better
                min(100, (symbols_count / len(self.config.symbols)) * 100),  # All symbols present = better
                min(100, sum(target_coverage.values()) / len(target_coverage) if target_coverage else 0),  # Target coverage
                100 if total_rows > self.config.min_observations_per_symbol * symbols_count else 50  # Sufficient data
            ]
            
            data_quality_score = sum(quality_factors) / len(quality_factors)
            
            return DatasetMetrics(
                total_rows=total_rows,
                total_features=total_features,
                symbols_count=symbols_count,
                date_range=date_range,
                target_coverage=target_coverage,
                data_quality_score=round(data_quality_score, 2),
                missing_data_percentage=round(missing_percentage, 2),
                feature_breakdown=feature_breakdown
            )
            
        except Exception as e:
            logger.error(f"‚ùå Error calculating metrics: {e}")
            return DatasetMetrics(
                total_rows=0, total_features=0, symbols_count=0,
                date_range=("N/A", "N/A"), target_coverage={},
                data_quality_score=0.0, missing_data_percentage=100.0,
                feature_breakdown={}
            )
    
    def _log_final_summary(self, datasets: Dict[DatasetType, pd.DataFrame]):
        """Log final summary of all created datasets"""
        logger.info("üìä FINAL DATASET CREATION SUMMARY")
        logger.info("=" * 70)
        
        for dataset_type, data in datasets.items():
            metrics = self._calculate_dataset_metrics(data, dataset_type)
            
            logger.info(f"{dataset_type.value.upper()} DATASET:")
            logger.info(f"   üìä Rows: {metrics.total_rows:,}")
            logger.info(f"   üîß Features: {metrics.total_features}")
            logger.info(f"   üè∑Ô∏è Symbols: {metrics.symbols_count}/{len(self.config.symbols)}")
            logger.info(f"   üìÖ Date Range: {metrics.date_range[0]} to {metrics.date_range[1]}")
            
            if metrics.target_coverage:
                primary_coverage = metrics.target_coverage.get('target_5', 0)
                logger.info(f"   üéØ Target_5 Coverage: {primary_coverage}%")
            
            logger.info(f"   üìà Quality Score: {metrics.data_quality_score}/100")
            
            # Feature breakdown
            breakdown = metrics.feature_breakdown
            logger.info(f"   üìã Features: Stock({breakdown.get('stock', 0)}), "
                       f"Technical({breakdown.get('technical', 0)}), "
                       f"Sentiment({breakdown.get('sentiment', 0)}), "
                       f"Decay({breakdown.get('temporal_decay', 0)}), "
                       f"Targets({breakdown.get('targets', 0)})")
            logger.info("")
        
        # Model recommendations
        logger.info("ü§ñ MODEL RECOMMENDATIONS:")
        if DatasetType.CORE in datasets:
            logger.info("   üìä LSTM / TFT Baseline ‚Üí Use CORE dataset (maximum data retention)")
        if DatasetType.SENTIMENT in datasets:
            logger.info("   üì∞ TFT Sentiment ‚Üí Use SENTIMENT dataset (core + sentiment)")
        if DatasetType.TEMPORAL_DECAY in datasets:
            logger.info("   ‚è∞ TFT Temporal Decay ‚Üí Use TEMPORAL_DECAY dataset (core + sentiment + decay)")
        
        logger.info("=" * 70)

# =============================================================================
# PUBLIC API FUNCTIONS
# =============================================================================

def load_dataset(dataset_type: DatasetType = DatasetType.CORE) -> pd.DataFrame:
    """Load a specific dataset type"""
    file_mapping = {
        DatasetType.CORE: CORE_DATASET,
        DatasetType.SENTIMENT: SENTIMENT_DATASET,
        DatasetType.TEMPORAL_DECAY: TEMPORAL_DECAY_DATASET
    }
    
    file_path = file_mapping.get(dataset_type, CORE_DATASET)
    
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"‚ùå Dataset not found: {file_path}")
    
    try:
        data = pd.read_csv(file_path, index_col=0, parse_dates=True)
        logger.info(f"üìä Loaded {dataset_type.value} dataset: {data.shape}")
        return data
    except Exception as e:
        logger.error(f"‚ùå Failed to load {dataset_type.value} dataset: {e}")
        raise

def get_dataset_for_model(model_type: str) -> pd.DataFrame:
    """Get appropriate dataset for a specific model type"""
    model_dataset_mapping = {
        'lstm': DatasetType.CORE,
        'tft_baseline': DatasetType.CORE,
        'tft_sentiment': DatasetType.SENTIMENT,
        'tft_temporal_decay': DatasetType.TEMPORAL_DECAY,
        'baseline_tft': DatasetType.CORE,  # Alias
        'sentiment_tft': DatasetType.SENTIMENT,  # Alias  
        'decay_tft': DatasetType.TEMPORAL_DECAY,  # Alias
    }
    
    dataset_type = model_dataset_mapping.get(model_type.lower(), DatasetType.CORE)
    logger.info(f"ü§ñ Model '{model_type}' ‚Üí {dataset_type.value} dataset")
    return load_dataset(dataset_type)

def collect_complete_dataset(config: Dict[str, Any]) -> pd.DataFrame:
    """
    Main function for complete dataset collection - creates all variants
    ORCHESTRATED: Now creates multiple dataset types efficiently
    """
    logger.info("üöÄ COLLECTING COMPLETE DATASET (MULTIPLE VARIANTS)")
    logger.info("=" * 70)
    
    try:
        # Convert config to DatasetConfig
        dataset_config = DatasetConfig(
            symbols=config['data']['symbols'],
            start_date=config['data']['start_date'],
            end_date=config['data']['end_date'],
            target_horizons=config['data'].get('target_horizons', [5, 30, 90]),
            fnspid_data_file=config['data'].get('fnspid_data_dir', FNSPID_DATA_FILE),
            include_sentiment=config.get('include_sentiment', True),
            include_temporal_decay=config.get('include_temporal_decay', True),
            cache_enabled=config.get('cache_enabled', True),
            decay_lambda=config.get('decay_lambda', 0.94),  # RiskMetrics standard
            max_decay_days=config.get('max_decay_days', 90)
        )
        
        # Create orchestrator and build all datasets
        orchestrator = DatasetOrchestrator(dataset_config)
        datasets = orchestrator.create_all_datasets()
        
        if not datasets:
            raise ValueError("‚ùå No datasets created")
        
        # Return the most complete dataset for backward compatibility
        if DatasetType.TEMPORAL_DECAY in datasets:
            return datasets[DatasetType.TEMPORAL_DECAY]
        elif DatasetType.SENTIMENT in datasets:
            return datasets[DatasetType.SENTIMENT]
        else:
            return datasets[DatasetType.CORE]
        
    except Exception as e:
        logger.error(f"‚ùå Complete dataset collection failed: {e}")
        raise

def get_data_summary(data: pd.DataFrame) -> Dict[str, Any]:
    """Generate comprehensive data summary"""
    logger.info("üìä Generating data summary...")
    
    try:
        if data is None or data.empty:
            return {'error': 'Data is empty', 'total_rows': 0, 'total_columns': 0}
        
        summary = {
            'total_rows': len(data),
            'total_columns': len(data.columns),
            'memory_usage_mb': round(data.memory_usage(deep=True).sum() / (1024 * 1024), 2)
        }
        
        # Date range
        date_col = None
        if 'date' in data.columns:
            date_col = data['date']
        elif isinstance(data.index, pd.DatetimeIndex):
            date_col = data.index
        
        if date_col is not None:
            summary['date_range'] = {
                'start': str(date_col.min().date()) if pd.notna(date_col.min()) else 'N/A',
                'end': str(date_col.max().date()) if pd.notna(date_col.max()) else 'N/A',
                'total_days': int((date_col.max() - date_col.min()).days) if pd.notna(date_col.min()) else 0
            }
        
        # Symbol analysis
        if 'symbol' in data.columns:
            symbol_counts = data['symbol'].value_counts()
            summary['symbols'] = {
                'count': len(symbol_counts),
                'list': list(symbol_counts.index),
                'distribution': {str(k): int(v) for k, v in symbol_counts.to_dict().items()}
            }
        
        # Feature categorization
        all_columns = list(data.columns)
        numeric_cols = list(data.select_dtypes(include=[np.number]).columns)
        target_cols = [col for col in all_columns if col.startswith('target_')]
        sentiment_cols = [col for col in all_columns if any(
            word in col.lower() for word in ['sentiment', 'news', 'content', 'finbert']
        )]
        technical_cols = [col for col in numeric_cols if any(
            word in col.lower() for word in ['ema_', 'sma_', 'rsi', 'macd', 'bb_', 'vwap', 'atr', 'roc_']
        )]
        decay_cols = [col for col in all_columns if '_decay_' in col]
        
        summary['columns'] = {
            'numeric_columns': len(numeric_cols),
            'target_columns': len(target_cols),
            'sentiment_columns': len(sentiment_cols),
            'technical_columns': len(technical_cols),
            'temporal_decay_columns': len(decay_cols),
            'targets': target_cols,
            'sentiment': sentiment_cols,
            'technical': technical_cols[:10],  # Show first 10 only
            'temporal_decay': decay_cols[:10]  # Show first 10 only
        }
        
        # Data quality
        missing_data = data.isnull().sum()
        total_cells = len(data) * len(data.columns)
        
        summary['data_quality'] = {
            'missing_values': int(missing_data.sum()),
            'missing_percentage': round((missing_data.sum() / total_cells) * 100, 2) if total_cells > 0 else 0.0,
            'columns_with_missing': {str(k): int(v) for k, v in missing_data[missing_data > 0].to_dict().items()}
        }
        
        # Target statistics
        if target_cols:
            target_stats = {}
            for col in target_cols:
                if col in data.columns:
                    target_data = data[col].dropna()
                    if len(target_data) > 0:
                        target_stats[col] = {
                            'mean': round(target_data.mean(), 4),
                            'std': round(target_data.std(), 4),
                            'min': round(target_data.min(), 4),
                            'max': round(target_data.max(), 4),
                            'valid_samples': len(target_data),
                            'coverage_percentage': round((len(target_data) / len(data)) * 100, 2)
                        }
            summary['target_statistics'] = target_stats
        
        # Dataset type detection
        dataset_types = []
        if technical_cols:
            dataset_types.append('Core (Technical)')
        if sentiment_cols:
            dataset_types.append('Sentiment')
        if decay_cols:
            dataset_types.append('Temporal Decay')
        
        summary['detected_dataset_types'] = dataset_types
        
        return summary
        
    except Exception as e:
        logger.error(f"‚ùå Error generating summary: {e}")
        return {
            'total_rows': 0,
            'total_columns': 0,
            'error': str(e),
            'error_type': type(e).__name__
        }

# Utility functions for dataset management
def check_dataset_exists(dataset_type: DatasetType = DatasetType.CORE) -> bool:
    """Check if a dataset exists"""
    file_mapping = {
        DatasetType.CORE: CORE_DATASET,
        DatasetType.SENTIMENT: SENTIMENT_DATASET,
        DatasetType.TEMPORAL_DECAY: TEMPORAL_DECAY_DATASET
    }
    
    file_path = file_mapping.get(dataset_type, CORE_DATASET)
    return os.path.exists(file_path)

def get_dataset_info(dataset_type: DatasetType = DatasetType.CORE) -> Dict[str, Any]:
    """Get basic info about a dataset"""
    file_mapping = {
        DatasetType.CORE: CORE_DATASET,
        DatasetType.SENTIMENT: SENTIMENT_DATASET,
        DatasetType.TEMPORAL_DECAY: TEMPORAL_DECAY_DATASET
    }
    
    file_path = file_mapping.get(dataset_type, CORE_DATASET)
    
    if not os.path.exists(file_path):
        return {'exists': False, 'path': file_path, 'dataset_type': dataset_type.value}
    
    try:
        stat = os.stat(file_path)
        return {
            'exists': True,
            'path': file_path,
            'dataset_type': dataset_type.value,
            'size_mb': round(stat.st_size / (1024 * 1024), 2),
            'modified': datetime.fromtimestamp(stat.st_mtime).isoformat(),
        }
    except Exception as e:
        return {'exists': True, 'path': file_path, 'error': str(e)}

def list_all_datasets() -> Dict[str, Dict[str, Any]]:
    """List information about all available datasets"""
    datasets_info = {}
    
    for dataset_type in DatasetType:
        info = get_dataset_info(dataset_type)
        datasets_info[dataset_type.value] = info
    
    return datasets_info

def get_available_models_and_datasets() -> Dict[str, str]:
    """Get mapping of available models to their recommended datasets"""
    available_datasets = {dt.value for dt in DatasetType if check_dataset_exists(dt)}
    
    models_datasets = {}
    
    # Core dataset models
    if 'core' in available_datasets:
        models_datasets.update({
            'lstm': 'core',
            'tft_baseline': 'core',
            'baseline_tft': 'core'
        })
    
    # Sentiment dataset models
    if 'sentiment' in available_datasets:
        models_datasets.update({
            'tft_sentiment': 'sentiment',
            'sentiment_tft': 'sentiment'
        })
    
    # Temporal decay dataset models
    if 'temporal_decay' in available_datasets:
        models_datasets.update({
            'tft_temporal_decay': 'temporal_decay',
            'decay_tft': 'temporal_decay'
        })
    
    return models_datasets

# =============================================================================
# MAIN EXECUTION AND TESTING
# =============================================================================

if __name__ == "__main__":
    # Test the comprehensive data collection
    print("üß™ TESTING FINAL DATA.PY IMPLEMENTATION")
    print("=" * 70)
    
    try:
        # Test configuration with your specifications
        test_config = {
            'data': {
                'symbols': ['AAPL', 'MSFT'],  # Start with 2 symbols for testing
                'start_date': '2023-01-01',
                'end_date': '2023-06-30',
                'target_horizons': [5, 30],
                'fnspid_data_dir': FNSPID_DATA_FILE  # Your FNSPID file location
            },
            'include_sentiment': True,
            'include_temporal_decay': True,
            'cache_enabled': True,
            'decay_lambda': 0.94,  # RiskMetrics standard
            'max_decay_days': 90
        }
        
        print(f"üìä Test config:")
        print(f"   Symbols: {test_config['data']['symbols']}")
        print(f"   Date range: {test_config['data']['start_date']} to {test_config['data']['end_date']}")
        print(f"   FNSPID file: {test_config['data']['fnspid_data_dir']}")
        print(f"   Decay lambda: {test_config['decay_lambda']} (RiskMetrics standard)")
        
        # Test dataset collection
        print("\nüìä Testing dataset collection...")
        dataset = collect_complete_dataset(test_config)
        
        print(f"‚úÖ Dataset collected: {dataset.shape}")
        
        # Test individual dataset loading
        print("\nüìã Testing individual dataset types...")
        for dataset_type in DatasetType:
            if check_dataset_exists(dataset_type):
                info = get_dataset_info(dataset_type)
                print(f"   ‚úÖ {dataset_type.value}: {info['size_mb']} MB")
            else:
                print(f"   ‚ùå {dataset_type.value}: Not available")
        
        # Test model-specific dataset loading
        print("\nü§ñ Testing model-specific datasets...")
        models_datasets = get_available_models_and_datasets()
        
        for model_type, dataset_type in models_datasets.items():
            try:
                model_data = get_dataset_for_model(model_type)
                print(f"   ‚úÖ {model_type} ‚Üí {dataset_type}: {model_data.shape}")
            except FileNotFoundError:
                print(f"   ‚ùå {model_type} ‚Üí {dataset_type}: Dataset not available")
        
        # Test data summary
        print("\nüìä Testing data summary...")
        summary = get_data_summary(dataset)
        print(f"   Dataset types detected: {summary.get('detected_dataset_types', [])}")
        print(f"   Technical indicators: {summary['columns']['technical_columns']}")
        print(f"   Sentiment features: {summary['columns']['sentiment_columns']}")
        print(f"   Temporal decay features: {summary['columns']['temporal_decay_columns']}")
        
        if 'target_statistics' in summary:
            target_stats = summary['target_statistics']
            if 'target_5' in target_stats:
                coverage = target_stats['target_5']['coverage_percentage']
                print(f"   Target_5 coverage: {coverage}% (FIXED!)")
        
        print("\n‚úÖ FINAL DATA.PY TEST COMPLETED SUCCESSFULLY!")
        print("\nKey Features Implemented:")
        print("‚úÖ Multiple dataset variants (Core, Sentiment, Temporal Decay)")
        print("‚úÖ Fixed target variable calculation")
        print("‚úÖ Your specified technical indicators (OHLCV + MACD + EMA + VWAP + BB + RSI + Optional)")
        print("‚úÖ RiskMetrics standard exponential decay (Œª=0.94)")
        print("‚úÖ FNSPID sentiment data integration")
        print("‚úÖ Model-specific dataset orchestration")
        print("‚úÖ Comprehensive caching and error handling")
        
        print("\nNext Steps:")
        print("1. Run your full dataset: python -c \"from src.data import collect_complete_dataset; collect_complete_dataset(your_config)\"")
        print("2. Test with your symbols and date range")
        print("3. Update models.py to use get_dataset_for_model()")
        print("4. Update run_experiment.py integration")
        
    except Exception as e:
        print(f"‚ùå Test failed: {e}")
        import traceback
        traceback.print_exc()
        
        print("\nTroubleshooting:")
        print("1. Check if required libraries are installed: pip install ta yfinance pandas numpy")
        print("2. Verify FNSPID data file exists at data/raw/nasdaq_exteral_data.csv")
        print("3. Check internet connection for Yahoo Finance data")
        print("4. Run with smaller symbol list or date range")