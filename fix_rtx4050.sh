#!/bin/bash

# ============================================================================
# COMPLETE RTX 4050 CUDA FIX SCRIPT
# Run this script to fix all CUDA/packaging issues automatically
# ============================================================================

set -e  # Exit on any error

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# Function to print colored output
print_status() {
    echo -e "${GREEN}âœ… $1${NC}"
}

print_warning() {
    echo -e "${YELLOW}âš ï¸ $1${NC}"
}

print_error() {
    echo -e "${RED}âŒ $1${NC}"
}

print_info() {
    echo -e "${BLUE}â„¹ï¸ $1${NC}"
}

print_step() {
    echo -e "${PURPLE}ğŸ”§ $1${NC}"
}

print_gpu() {
    echo -e "${CYAN}ğŸ® $1${NC}"
}

# Function to run commands with error handling
run_command() {
    local cmd="$1"
    local description="$2"
    
    print_step "$description"
    echo "Command: $cmd"
    echo "----------------------------------------"
    
    if eval "$cmd"; then
        print_status "Success: $description"
        echo ""
        return 0
    else
        print_error "Failed: $description"
        echo ""
        return 1
    fi
}

# Function to check if command exists
command_exists() {
    command -v "$1" >/dev/null 2>&1
}

# Main script starts here
clear
echo "ğŸ® RTX 4050 CUDA COMPLETE FIX SCRIPT"
echo "===================================="
echo ""
echo "This script will:"
echo "1. ğŸ—‘ï¸  Uninstall conflicting packages"
echo "2. ğŸ“¦ Install compatible packaging version"  
echo "3. ğŸ® Install CUDA PyTorch for RTX 4050"
echo "4. âš¡ Install PyTorch Lightning"
echo "5. ğŸ“Š Install essential ML packages"
echo "6. ğŸ” Verify CUDA installation"
echo "7. âš¡ Test GPU performance"
echo "8. ğŸ“ Create test script"
echo ""
echo "===================================="
echo ""

# Check if pip exists
if ! command_exists pip; then
    print_error "pip not found! Please install Python with pip first."
    exit 1
fi

print_info "Python version: $(python --version)"
print_info "Pip version: $(pip --version)"
echo ""

# Ask for confirmation
read -p "ğŸš€ Ready to fix RTX 4050 CUDA installation? (y/n): " -n 1 -r
echo ""
if [[ ! $REPLY =~ ^[Yy]$ ]]; then
    print_warning "Installation cancelled."
    exit 0
fi

echo ""
print_step "Starting RTX 4050 CUDA fix..."
echo ""

# Step 1: Uninstall conflicting packages
print_step "STEP 1: Removing conflicting packages"
echo "======================================"

packages_to_remove="torch torchvision torchaudio lightning packaging"
run_command "pip uninstall $packages_to_remove -y" "Uninstalling conflicting packages"

# Step 2: Install compatible packaging
print_step "STEP 2: Installing compatible packaging"
echo "======================================"

run_command 'pip install "packaging>=20.0,<25.0"' "Installing compatible packaging version"

# Step 3: Install CUDA PyTorch
print_step "STEP 3: Installing CUDA PyTorch for RTX 4050"
echo "==========================================="

cuda_install_cmd='pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121'

if ! run_command "$cuda_install_cmd" "Installing CUDA PyTorch 12.1"; then
    print_warning "CUDA 12.1 failed, trying CUDA 11.8..."
    cuda_install_cmd_alt='pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118'
    run_command "$cuda_install_cmd_alt" "Installing CUDA PyTorch 11.8"
fi

# Step 4: Install PyTorch Lightning
print_step "STEP 4: Installing PyTorch Lightning"
echo "===================================="

run_command 'pip install "lightning>=2.4.0"' "Installing compatible PyTorch Lightning"

# Step 5: Install essential packages
print_step "STEP 5: Installing essential ML packages"  
echo "======================================="

essential_packages=(
    '"pandas>=1.5.0"'
    '"numpy>=1.21.0"'
    '"scikit-learn>=1.1.0"'
    '"matplotlib>=3.5.0"'
    '"seaborn>=0.11.0"'
    '"jupyter>=1.0.0"'
    '"tqdm"'
    '"pillow"'
)

for package in "${essential_packages[@]}"; do
    run_command "pip install $package" "Installing $package"
done

# Step 6: Verify installation
print_step "STEP 6: Verifying CUDA installation"
echo "==================================="

# Create verification script
cat > verify_cuda.py << 'EOF'
import sys
import traceback

def test_imports():
    """Test basic imports."""
    try:
        import torch
        import torchvision
        import torchaudio
        import lightning
        import packaging
        print("âœ… All imports successful")
        return True
    except Exception as e:
        print(f"âŒ Import failed: {e}")
        return False

def test_cuda():
    """Test CUDA functionality."""
    try:
        import torch
        
        print(f"PyTorch version: {torch.__version__}")
        print(f"CUDA available: {torch.cuda.is_available()}")
        
        if torch.cuda.is_available():
            print(f"CUDA version: {torch.version.cuda}")
            print(f"GPU count: {torch.cuda.device_count()}")
            
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
            
            print(f"GPU name: {gpu_name}")
            print(f"GPU memory: {gpu_memory:.1f} GB")
            
            if "4050" in gpu_name:
                print("ğŸ® RTX 4050 detected!")
            
            return True
        else:
            print("âŒ CUDA not available")
            return False
            
    except Exception as e:
        print(f"âŒ CUDA test failed: {e}")
        traceback.print_exc()
        return False

def test_gpu_computation():
    """Test actual GPU computation."""
    try:
        import torch
        import time
        
        if not torch.cuda.is_available():
            print("âš ï¸ Skipping GPU computation test (CUDA not available)")
            return False
        
        device = torch.device('cuda')
        print(f"Testing GPU computation on: {torch.cuda.get_device_name(0)}")
        
        # Warm up
        x = torch.randn(1000, 1000, device=device)
        y = torch.mm(x, x)
        torch.cuda.synchronize()
        
        # Performance test
        matrix_size = 3000
        print(f"Matrix multiplication test: {matrix_size}x{matrix_size}")
        
        start_time = time.time()
        x_gpu = torch.randn(matrix_size, matrix_size, device=device)
        y_gpu = torch.mm(x_gpu, x_gpu)
        torch.cuda.synchronize()
        gpu_time = time.time() - start_time
        
        print(f"âš¡ GPU computation time: {gpu_time:.3f}s")
        
        # Test mixed precision
        try:
            with torch.cuda.amp.autocast():
                z = torch.mm(x_gpu, y_gpu)
            print("âœ… Mixed precision (FP16): Working")
        except Exception as e:
            print(f"âš ï¸ Mixed precision: {e}")
        
        # Memory info
        memory_used = torch.cuda.memory_allocated() / 1e9
        memory_total = torch.cuda.get_device_properties(0).total_memory / 1e9
        print(f"ğŸ§  GPU memory used: {memory_used:.2f} / {memory_total:.1f} GB")
        
        # Cleanup
        del x, y, x_gpu, y_gpu
        torch.cuda.empty_cache()
        
        print("âœ… GPU computation test: PASSED")
        return True
        
    except Exception as e:
        print(f"âŒ GPU computation test failed: {e}")
        traceback.print_exc()
        return False

def main():
    print("ğŸ” CUDA VERIFICATION")
    print("=" * 30)
    
    # Test 1: Imports
    print("\nğŸ“¦ Testing imports...")
    imports_ok = test_imports()
    
    # Test 2: CUDA detection
    print("\nğŸ® Testing CUDA...")
    cuda_ok = test_cuda()
    
    # Test 3: GPU computation
    print("\nâš¡ Testing GPU computation...")
    gpu_ok = test_gpu_computation()
    
    # Summary
    print("\n" + "=" * 40)
    print("ğŸ“Š VERIFICATION SUMMARY")
    print("=" * 40)
    print(f"Imports: {'âœ… PASS' if imports_ok else 'âŒ FAIL'}")
    print(f"CUDA: {'âœ… PASS' if cuda_ok else 'âŒ FAIL'}")
    print(f"GPU Computation: {'âœ… PASS' if gpu_ok else 'âŒ FAIL'}")
    
    if imports_ok and cuda_ok and gpu_ok:
        print("\nğŸ‰ ALL TESTS PASSED!")
        print("ğŸš€ RTX 4050 is ready for training!")
        print("âš¡ Expected training speedup: 8-12x vs CPU")
        return True
    else:
        print("\nâŒ SOME TESTS FAILED")
        print("ğŸ”§ Check the error messages above")
        return False

if __name__ == "__main__":
    main()
EOF

# Run verification
print_info "Running CUDA verification..."
python verify_cuda.py

verification_exit_code=$?

echo ""

# Step 7: Create optimized config
print_step "STEP 7: Creating RTX 4050 optimized config"
echo "=========================================="

cat > rtx4050_config.py << 'EOF'
"""
RTX 4050 Optimized Configuration for Financial ML Training
Use this config in your training scripts for optimal performance
"""

import torch

# Check if CUDA is available
CUDA_AVAILABLE = torch.cuda.is_available()

if CUDA_AVAILABLE:
    # RTX 4050 Optimized Settings
    RTX_4050_CONFIG = {
        'device': 'cuda',
        'batch_size': 128,              # Optimal for 6GB VRAM
        'sequence_length': 60,          # Good for financial time series
        'hidden_size': 128,             # Balanced performance/memory
        'num_attention_heads': 8,       # Optimal for RTX 4050
        'num_layers': 3,                # Deep enough without overfitting
        'dropout': 0.1,
        'learning_rate': 0.001,
        'epochs': 50,                   # Real training epochs
        'early_stopping_patience': 10,
        'mixed_precision': True,        # FP16 for 50% speedup
        'grad_clip': 1.0,
        'weight_decay': 0.01,
        
        # RTX 4050 specific optimizations
        'num_workers': 4,               # DataLoader workers
        'pin_memory': True,             # Faster GPU transfer
        'memory_fraction': 0.9,         # Use 90% of VRAM
        'benchmark_cudnn': True,        # Optimize for consistent input sizes
    }
    
    print("ğŸ® RTX 4050 Config Loaded:")
    print(f"   Device: {RTX_4050_CONFIG['device']}")
    print(f"   Batch Size: {RTX_4050_CONFIG['batch_size']}")
    print(f"   Mixed Precision: {RTX_4050_CONFIG['mixed_precision']}")
    print(f"   Expected Training Time: 15-25 minutes")
    print(f"   Expected Speedup vs CPU: 8-12x")
    
else:
    # CPU Fallback
    RTX_4050_CONFIG = {
        'device': 'cpu',
        'batch_size': 32,
        'sequence_length': 30,
        'hidden_size': 64,
        'num_attention_heads': 4,
        'num_layers': 2,
        'dropout': 0.1,
        'learning_rate': 0.001,
        'epochs': 20,
        'early_stopping_patience': 5,
        'mixed_precision': False,
        'grad_clip': 1.0,
        'weight_decay': 0.01,
        'num_workers': 0,
        'pin_memory': False,
    }
    
    print("ğŸ’» CPU Fallback Config Loaded")
    print("âš ï¸ CUDA not available - using CPU settings")

# Export config
CONFIG = RTX_4050_CONFIG

# Helper function to apply config
def setup_rtx4050():
    """Setup RTX 4050 optimizations."""
    if CUDA_AVAILABLE:
        # Set memory fraction
        torch.cuda.set_per_process_memory_fraction(CONFIG['memory_fraction'])
        
        # Enable cuDNN benchmark for performance
        if CONFIG['benchmark_cudnn']:
            torch.backends.cudnn.benchmark = True
        
        print("âœ… RTX 4050 optimizations applied")
        return torch.device('cuda')
    else:
        print("âš ï¸ CUDA not available")
        return torch.device('cpu')

if __name__ == "__main__":
    device = setup_rtx4050()
    print(f"Device: {device}")
EOF

print_status "Created rtx4050_config.py - use this in your training scripts!"

# Step 8: Create training template
print_step "STEP 8: Creating training template"
echo "================================="

cat > train_template.py << 'EOF'
"""
RTX 4050 Training Template
Copy this code into your training script for optimal performance
"""

import torch
import torch.nn as nn
from rtx4050_config import CONFIG, setup_rtx4050

def create_model(input_size):
    """Create model optimized for RTX 4050."""
    device = setup_rtx4050()
    
    # Your model definition here
    model = YourModel(
        input_size=input_size,
        hidden_size=CONFIG['hidden_size'],
        num_heads=CONFIG['num_attention_heads'],
        num_layers=CONFIG['num_layers'],
        dropout=CONFIG['dropout']
    ).to(device)
    
    return model, device

def create_data_loader(dataset):
    """Create optimized data loader for RTX 4050."""
    return torch.utils.data.DataLoader(
        dataset,
        batch_size=CONFIG['batch_size'],
        shuffle=True,
        num_workers=CONFIG['num_workers'],
        pin_memory=CONFIG['pin_memory'],
        persistent_workers=CONFIG['num_workers'] > 0
    )

def train_with_rtx4050(model, train_loader, val_loader):
    """Training function optimized for RTX 4050."""
    device = setup_rtx4050()
    
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=CONFIG['learning_rate'],
        weight_decay=CONFIG['weight_decay']
    )
    
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', patience=5, factor=0.5
    )
    
    criterion = nn.MSELoss()
    
    # Mixed precision scaler for RTX 4050
    scaler = torch.cuda.amp.GradScaler() if CONFIG['mixed_precision'] else None
    
    for epoch in range(CONFIG['epochs']):
        model.train()
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)
            
            optimizer.zero_grad()
            
            if CONFIG['mixed_precision'] and scaler:
                # Mixed precision training for RTX 4050
                with torch.cuda.amp.autocast():
                    output = model(data)
                    loss = criterion(output, target)
                
                scaler.scale(loss).backward()
                
                if CONFIG['grad_clip'] > 0:
                    scaler.unscale_(optimizer)
                    torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG['grad_clip'])
                
                scaler.step(optimizer)
                scaler.update()
            else:
                # Standard training
                output = model(data)
                loss = criterion(output, target)
                loss.backward()
                
                if CONFIG['grad_clip'] > 0:
                    torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG['grad_clip'])
                
                optimizer.step()
            
            # GPU memory management
            if device.type == 'cuda':
                torch.cuda.empty_cache()
        
        # Validation and scheduler step
        # ... your validation code here ...
        
        scheduler.step(val_loss)

print("ğŸ“ RTX 4050 training template created!")
print("ğŸ’¡ Copy the functions above into your training script")
EOF

print_status "Created train_template.py - copy functions into your training script!"

# Final summary
echo ""
print_step "INSTALLATION COMPLETE!"
echo "======================"

if [ $verification_exit_code -eq 0 ]; then
    print_gpu "ğŸ‰ RTX 4050 CUDA INSTALLATION SUCCESSFUL!"
    echo ""
    print_status "What's ready:"
    echo "   âœ… CUDA PyTorch installed"
    echo "   âœ… RTX 4050 detected and working"
    echo "   âœ… Mixed precision (FP16) enabled"
    echo "   âœ… All dependencies compatible"
    echo ""
    print_gpu "Performance improvements:"
    echo "   ğŸš€ Training speed: 8-12x faster than CPU"
    echo "   ğŸ“Š Batch size: 128 (vs 32 on CPU)"
    echo "   âš¡ Mixed precision: +50% speedup"
    echo "   â±ï¸ Expected training time: 15-25 minutes"
    echo ""
    print_info "Generated files:"
    echo "   ğŸ“„ verify_cuda.py - CUDA verification script"
    echo "   âš™ï¸ rtx4050_config.py - Optimized configuration"
    echo "   ğŸ“ train_template.py - Training template"
    echo ""
    print_gpu "ğŸš€ RE-RUN YOUR TRAINING SCRIPT NOW!"
    echo "   It will automatically use RTX 4050 configuration"
else
    print_error "âŒ CUDA verification failed!"
    echo ""
    print_warning "Possible fixes:"
    echo "   1. Restart Python/Jupyter kernel"
    echo "   2. Check NVIDIA drivers: nvidia-smi"
    echo "   3. Try alternative CUDA version:"
    echo "      pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
    echo ""
    print_info "You can still run the training on CPU (slower)"
fi

echo ""
print_info "ğŸ” To test anytime, run: python verify_cuda.py"
print_info "âš™ï¸ To see config, run: python rtx4050_config.py"
echo ""
print_step "RTX 4050 setup script completed!"