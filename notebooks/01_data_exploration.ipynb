{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12b878a6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bfb7441",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas_ta'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmanifold\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TSNE\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Time series libraries\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas_ta\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mta\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime, timedelta\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Configuration\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas_ta'"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Statistical libraries\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Time series libraries\n",
    "import pandas_ta as ta\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "# Set figure parameters\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"üìö Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ac8bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset created by Step 1\n",
    "try:\n",
    "    df = pd.read_parquet('data/processed/combined_dataset.parquet')\n",
    "    print(f\"‚úÖ Dataset loaded successfully!\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Date range: {df.index.min()} to {df.index.max()}\")\n",
    "    print(f\"Symbols: {df['symbol'].unique()}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Dataset not found. Please run Step 1 first:\")\n",
    "    print(\"python run_experiment.py --step 1\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5719b291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_overview(df):\n",
    "    \"\"\"Comprehensive dataset overview\"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"üìä DATASET OVERVIEW\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Basic info\n",
    "    print(f\"üìÖ Date Range: {df.index.min().date()} to {df.index.max().date()}\")\n",
    "    print(f\"üìä Shape: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "    print(f\"üìà Symbols: {len(df['symbol'].unique())} ({', '.join(df['symbol'].unique())})\")\n",
    "    \n",
    "    if 'sector' in df.columns:\n",
    "        print(f\"üè¢ Sectors: {len(df['sector'].unique())} ({', '.join(df['sector'].unique())})\")\n",
    "    \n",
    "    # Memory usage\n",
    "    memory_mb = df.memory_usage(deep=True).sum() / 1024 / 1024\n",
    "    print(f\"üíæ Memory Usage: {memory_mb:.1f} MB\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 40)\n",
    "    print(\"üìã COLUMN BREAKDOWN\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Categorize columns\n",
    "    feature_groups = {\n",
    "        'Price Features': [col for col in df.columns if col in ['Open', 'High', 'Low', 'Close', 'Volume']],\n",
    "        'Technical Features': [col for col in df.columns if any(tech in col for tech in ['SMA', 'EMA', 'RSI', 'MACD', 'BB', 'volatility'])],\n",
    "        'News Features': [col for col in df.columns if 'news' in col.lower()],\n",
    "        'Lag Features': [col for col in df.columns if 'lag' in col.lower()],\n",
    "        'Target Features': [col for col in df.columns if col.startswith(('target_', 'return_', 'direction_'))],\n",
    "        'Meta Features': [col for col in df.columns if col in ['symbol', 'sector']]\n",
    "    }\n",
    "    \n",
    "    for group, features in feature_groups.items():\n",
    "        if features:\n",
    "            print(f\"{group}: {len(features)}\")\n",
    "            print(f\"  ‚îî‚îÄ {', '.join(features[:5])}{'...' if len(features) > 5 else ''}\")\n",
    "    \n",
    "    print(f\"\\nTotal Features: {sum(len(features) for features in feature_groups.values())}\")\n",
    "    \n",
    "    return feature_groups\n",
    "\n",
    "# Run overview\n",
    "feature_groups = dataset_overview(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44b37fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality assessment\n",
    "def assess_data_quality(df):\n",
    "    \"\"\"Assess data quality across all features\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üîç DATA QUALITY ASSESSMENT\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Missing values\n",
    "    missing_summary = df.isnull().sum()\n",
    "    missing_pct = (missing_summary / len(df)) * 100\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing_Count': missing_summary,\n",
    "        'Missing_Pct': missing_pct\n",
    "    }).sort_values('Missing_Pct', ascending=False)\n",
    "    \n",
    "    # Show columns with missing values\n",
    "    has_missing = missing_df[missing_df['Missing_Count'] > 0]\n",
    "    if not has_missing.empty:\n",
    "        print(\"‚ö†Ô∏è COLUMNS WITH MISSING VALUES:\")\n",
    "        print(has_missing.head(10))\n",
    "    else:\n",
    "        print(\"‚úÖ No missing values found!\")\n",
    "    \n",
    "    # Data types\n",
    "    print(f\"\\nüìä DATA TYPES:\")\n",
    "    dtype_counts = df.dtypes.value_counts()\n",
    "    for dtype, count in dtype_counts.items():\n",
    "        print(f\"  {dtype}: {count} columns\")\n",
    "    \n",
    "    # Basic statistics for numerical columns\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    print(f\"\\nüìà NUMERICAL FEATURES: {len(numerical_cols)}\")\n",
    "    \n",
    "    # Identify potential outliers (using IQR method)\n",
    "    outlier_summary = {}\n",
    "    for col in numerical_cols[:10]:  # Check first 10 numerical columns\n",
    "        if col not in ['symbol']:\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            outliers = df[(df[col] < Q1 - 1.5*IQR) | (df[col] > Q3 + 1.5*IQR)][col]\n",
    "            outlier_summary[col] = len(outliers)\n",
    "    \n",
    "    outlier_df = pd.DataFrame(list(outlier_summary.items()), \n",
    "                             columns=['Feature', 'Outlier_Count']).sort_values('Outlier_Count', ascending=False)\n",
    "    print(f\"\\nüéØ OUTLIERS (Top 5):\")\n",
    "    print(outlier_df.head())\n",
    "\n",
    "assess_data_quality(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93350aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_price_movements(df):\n",
    "    \"\"\"Analyze price movements and volatility patterns\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üí∞ PRICE MOVEMENT ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Calculate daily returns for each symbol\n",
    "    symbols = df['symbol'].unique()\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Plot 1: Price Evolution\n",
    "    ax1 = axes[0, 0]\n",
    "    for symbol in symbols:\n",
    "        symbol_data = df[df['symbol'] == symbol]['Close']\n",
    "        # Normalize to starting price for comparison\n",
    "        normalized_price = (symbol_data / symbol_data.iloc[0]) * 100\n",
    "        ax1.plot(normalized_price.index, normalized_price.values, label=symbol, alpha=0.8, linewidth=2)\n",
    "    \n",
    "    ax1.set_title('üìà Normalized Price Evolution (Base=100)', fontsize=14, fontweight='bold')\n",
    "    ax1.set_ylabel('Normalized Price')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Volume Patterns\n",
    "    ax2 = axes[0, 1]\n",
    "    for symbol in symbols:\n",
    "        symbol_data = df[df['symbol'] == symbol]['Volume']\n",
    "        # Rolling 30-day average volume\n",
    "        volume_ma = symbol_data.rolling(30).mean()\n",
    "        ax2.plot(volume_ma.index, volume_ma.values, label=symbol, alpha=0.8, linewidth=2)\n",
    "    \n",
    "    ax2.set_title('üìä Volume Trends (30-day MA)', fontsize=14, fontweight='bold')\n",
    "    ax2.set_ylabel('Volume (30-day MA)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Volatility Analysis\n",
    "    ax3 = axes[1, 0]\n",
    "    for symbol in symbols:\n",
    "        symbol_data = df[df['symbol'] == symbol]\n",
    "        if 'volatility_20d' in symbol_data.columns:\n",
    "            volatility = symbol_data['volatility_20d'] * 100  # Convert to percentage\n",
    "            ax3.plot(volatility.index, volatility.values, label=symbol, alpha=0.8, linewidth=2)\n",
    "    \n",
    "    ax3.set_title('üìâ 20-Day Volatility (%)', fontsize=14, fontweight='bold')\n",
    "    ax3.set_ylabel('Volatility (%)')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Return Distribution\n",
    "    ax4 = axes[1, 1]\n",
    "    all_returns = []\n",
    "    for symbol in symbols:\n",
    "        symbol_data = df[df['symbol'] == symbol]['Close']\n",
    "        returns = symbol_data.pct_change().dropna() * 100\n",
    "        all_returns.extend(returns.values)\n",
    "        ax4.hist(returns.values, bins=50, alpha=0.6, label=symbol, density=True)\n",
    "    \n",
    "    ax4.set_title('üìä Daily Return Distributions (%)', fontsize=14, fontweight='bold')\n",
    "    ax4.set_xlabel('Daily Return (%)')\n",
    "    ax4.set_ylabel('Density')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nüìä SUMMARY STATISTICS:\")\n",
    "    for symbol in symbols:\n",
    "        symbol_data = df[df['symbol'] == symbol]\n",
    "        returns = symbol_data['Close'].pct_change().dropna() * 100\n",
    "        \n",
    "        print(f\"\\n{symbol}:\")\n",
    "        print(f\"  üìà Total Return: {((symbol_data['Close'].iloc[-1] / symbol_data['Close'].iloc[0]) - 1) * 100:.1f}%\")\n",
    "        print(f\"  üìä Daily Return (Mean): {returns.mean():.3f}%\")\n",
    "        print(f\"  üìâ Daily Volatility: {returns.std():.3f}%\")\n",
    "        print(f\"  üìã Sharpe Ratio*: {returns.mean() / returns.std():.3f}\")\n",
    "        print(f\"  üéØ Max Daily Gain: {returns.max():.2f}%\")\n",
    "        print(f\"  ‚¨áÔ∏è Max Daily Loss: {returns.min():.2f}%\")\n",
    "\n",
    "analyze_price_movements(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c64a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive price chart with volume\n",
    "def create_interactive_price_chart(df):\n",
    "    \"\"\"Create interactive price and volume charts\"\"\"\n",
    "    \n",
    "    symbols = df['symbol'].unique()\n",
    "    \n",
    "    # Create dropdown for symbol selection\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=1,\n",
    "        shared_xaxes=True,\n",
    "        vertical_spacing=0.05,\n",
    "        subplot_titles=('Price (OHLC)', 'Volume'),\n",
    "        row_heights=[0.7, 0.3]\n",
    "    )\n",
    "    \n",
    "    # Use first symbol as default\n",
    "    symbol = symbols[0]\n",
    "    symbol_data = df[df['symbol'] == symbol].copy()\n",
    "    \n",
    "    # OHLC Candlestick\n",
    "    fig.add_trace(\n",
    "        go.Candlestick(\n",
    "            x=symbol_data.index,\n",
    "            open=symbol_data['Open'],\n",
    "            high=symbol_data['High'],\n",
    "            low=symbol_data['Low'],\n",
    "            close=symbol_data['Close'],\n",
    "            name='OHLC'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Add moving averages if available\n",
    "    if 'SMA_20' in symbol_data.columns:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=symbol_data.index,\n",
    "                y=symbol_data['SMA_20'],\n",
    "                name='SMA 20',\n",
    "                line=dict(color='orange', width=1)\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    if 'SMA_50' in symbol_data.columns:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=symbol_data.index,\n",
    "                y=symbol_data['SMA_50'],\n",
    "                name='SMA 50',\n",
    "                line=dict(color='red', width=1)\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    # Volume\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=symbol_data.index,\n",
    "            y=symbol_data['Volume'],\n",
    "            name='Volume',\n",
    "            marker_color='rgba(0,100,200,0.5)'\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f'üìà {symbol} Price and Volume Analysis',\n",
    "        xaxis_rangeslider_visible=False,\n",
    "        height=600,\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    print(f\"üìä Interactive chart shown for {symbol}\")\n",
    "    print(f\"üí° TIP: Modify the symbol variable to explore other stocks\")\n",
    "\n",
    "# Create interactive chart\n",
    "create_interactive_price_chart(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf4c1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_technical_indicators(df):\n",
    "    \"\"\"Analyze technical indicators and their effectiveness\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üîß TECHNICAL INDICATORS ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Get technical indicator columns\n",
    "    tech_indicators = [col for col in df.columns if any(tech in col for tech in \n",
    "                      ['RSI', 'MACD', 'BB', 'SMA', 'EMA', 'volatility'])]\n",
    "    \n",
    "    print(f\"üìä Found {len(tech_indicators)} technical indicators\")\n",
    "    \n",
    "    # Focus on key indicators for analysis\n",
    "    key_indicators = ['RSI_14', 'MACD', 'BB_upper_20', 'BB_lower_20', 'SMA_20', 'SMA_50', 'volatility_20d']\n",
    "    available_indicators = [ind for ind in key_indicators if ind in df.columns]\n",
    "    \n",
    "    if not available_indicators:\n",
    "        print(\"‚ö†Ô∏è No key technical indicators found in dataset\")\n",
    "        return\n",
    "    \n",
    "    symbols = df['symbol'].unique()\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Plot 1: RSI Distribution and Signals\n",
    "    if 'RSI_14' in df.columns:\n",
    "        ax1 = axes[0, 0]\n",
    "        for symbol in symbols:\n",
    "            symbol_data = df[df['symbol'] == symbol]['RSI_14'].dropna()\n",
    "            ax1.hist(symbol_data, bins=30, alpha=0.6, label=symbol, density=True)\n",
    "        \n",
    "        ax1.axvline(30, color='red', linestyle='--', label='Oversold (30)')\n",
    "        ax1.axvline(70, color='green', linestyle='--', label='Overbought (70)')\n",
    "        ax1.set_title('üìä RSI Distribution Analysis', fontsize=14, fontweight='bold')\n",
    "        ax1.set_xlabel('RSI Value')\n",
    "        ax1.set_ylabel('Density')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: MACD Histogram\n",
    "    if 'MACD' in df.columns:\n",
    "        ax2 = axes[0, 1]\n",
    "        symbol = symbols[0]  # Use first symbol for detailed view\n",
    "        symbol_data = df[df['symbol'] == symbol]\n",
    "        macd_data = symbol_data['MACD'].dropna()\n",
    "        ax2.plot(macd_data.index, macd_data.values, label='MACD', linewidth=1.5)\n",
    "        ax2.axhline(0, color='black', linestyle='-', alpha=0.5)\n",
    "        ax2.fill_between(macd_data.index, macd_data.values, 0, \n",
    "                        where=(macd_data.values >= 0), color='green', alpha=0.3, label='Positive')\n",
    "        ax2.fill_between(macd_data.index, macd_data.values, 0, \n",
    "                        where=(macd_data.values < 0), color='red', alpha=0.3, label='Negative')\n",
    "        ax2.set_title(f'üìà MACD Analysis - {symbol}', fontsize=14, fontweight='bold')\n",
    "        ax2.set_ylabel('MACD Value')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Bollinger Bands Analysis\n",
    "    if all(col in df.columns for col in ['BB_upper_20', 'BB_lower_20', 'Close']):\n",
    "        ax3 = axes[1, 0]\n",
    "        symbol = symbols[0]  # Use first symbol\n",
    "        symbol_data = df[df['symbol'] == symbol].tail(252)  # Last year\n",
    "        \n",
    "        ax3.plot(symbol_data.index, symbol_data['Close'], label='Close Price', linewidth=2)\n",
    "        ax3.plot(symbol_data.index, symbol_data['BB_upper_20'], label='BB Upper', alpha=0.7)\n",
    "        ax3.plot(symbol_data.index, symbol_data['BB_lower_20'], label='BB Lower', alpha=0.7)\n",
    "        ax3.fill_between(symbol_data.index, symbol_data['BB_upper_20'], symbol_data['BB_lower_20'], \n",
    "                        alpha=0.2, label='BB Range')\n",
    "        ax3.set_title(f'üìä Bollinger Bands - {symbol} (Last Year)', fontsize=14, fontweight='bold')\n",
    "        ax3.set_ylabel('Price')\n",
    "        ax3.legend()\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Volatility Patterns\n",
    "    if 'volatility_20d' in df.columns:\n",
    "        ax4 = axes[1, 1]\n",
    "        for symbol in symbols:\n",
    "            symbol_data = df[df['symbol'] == symbol]['volatility_20d'].dropna() * 100\n",
    "            ax4.plot(symbol_data.index, symbol_data.values, label=symbol, alpha=0.8, linewidth=2)\n",
    "        \n",
    "        ax4.set_title('üìâ Volatility Patterns (20-day)', fontsize=14, fontweight='bold')\n",
    "        ax4.set_ylabel('Volatility (%)')\n",
    "        ax4.legend()\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Technical indicator statistics\n",
    "    print(\"\\nüìä TECHNICAL INDICATOR STATISTICS:\")\n",
    "    for indicator in available_indicators:\n",
    "        values = df[indicator].dropna()\n",
    "        print(f\"\\n{indicator}:\")\n",
    "        print(f\"  Mean: {values.mean():.4f}\")\n",
    "        print(f\"  Std: {values.std():.4f}\")\n",
    "        print(f\"  Min: {values.min():.4f}\")\n",
    "        print(f\"  Max: {values.max():.4f}\")\n",
    "        \n",
    "        # Specific insights for each indicator\n",
    "        if 'RSI' in indicator:\n",
    "            oversold = (values < 30).sum()\n",
    "            overbought = (values > 70).sum()\n",
    "            print(f\"  üî¥ Oversold signals: {oversold} ({oversold/len(values)*100:.1f}%)\")\n",
    "            print(f\"  üü¢ Overbought signals: {overbought} ({overbought/len(values)*100:.1f}%)\")\n",
    "\n",
    "analyze_technical_indicators(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "614331d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  No highly correlated indicators found (all |r| ‚â§ 0.7)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 51\u001b[0m analyze_indicator_correlations(\u001b[43mdf\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Correlation analysis of technical indicators\n",
    "def analyze_indicator_correlations(df):\n",
    "    \"\"\"Analyze correlations between technical indicators\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üîó TECHNICAL INDICATOR CORRELATIONS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Get technical indicators\n",
    "    tech_cols = [col for col in df.columns if any(tech in col for tech in \n",
    "                ['RSI', 'MACD', 'SMA', 'EMA', 'BB', 'volatility'])]\n",
    "    \n",
    "    if len(tech_cols) < 2:\n",
    "        print(\"‚ö†Ô∏è Not enough technical indicators for correlation analysis\")\n",
    "        return\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = df[tech_cols].corr()\n",
    "    \n",
    "    # Create heatmap\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='RdYlBu_r', center=0,\n",
    "                square=True, linewidths=0.5, cbar_kws={\"shrink\": .8}, fmt='.2f')\n",
    "    plt.title('üîó Technical Indicators Correlation Matrix', fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find highly correlated pairs\n",
    "    print(\"\\nüîç HIGHLY CORRELATED INDICATORS (|r| > 0.7):\")\n",
    "    high_corr_pairs = []\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            corr_val = corr_matrix.iloc[i, j]\n",
    "            if abs(corr_val) > 0.7:\n",
    "                high_corr_pairs.append({\n",
    "                    'Indicator 1': corr_matrix.columns[i],\n",
    "                    'Indicator 2': corr_matrix.columns[j],\n",
    "                    'Correlation': corr_val\n",
    "                })\n",
    "    \n",
    "    if high_corr_pairs:\n",
    "        high_corr_df = pd.DataFrame(high_corr_pairs).sort_values('Correlation', key=abs, ascending=False)\n",
    "        for _, row in high_corr_df.iterrows():\n",
    "            print(f\"  {row['Indicator 1']} ‚Üî {row['Indicator 2']}: {row['Correlation']:.3f}\")\n",
    "    else:\n",
    "        print(\"  No highly correlated indicators found (all |r| ‚â§ 0.7)\")\n",
    "\n",
    "analyze_indicator_correlations(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63449d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable correlations across horizons\n",
    "def analyze_target_correlations(df):\n",
    "    \"\"\"Analyze correlations between different prediction horizons\"\"\"\n",
    "    \n",
    "    horizons = [5, 30, 90]\n",
    "    target_cols = [f'target_{h}d' for h in horizons if f'target_{h}d' in df.columns]\n",
    "    \n",
    "    if len(target_cols) < 2:\n",
    "        print(\"‚ö†Ô∏è Not enough target variables for correlation analysis\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 40)\n",
    "    print(\"üîó TARGET HORIZON CORRELATIONS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Calculate correlations\n",
    "    target_corr = df[target_cols].corr()\n",
    "    \n",
    "    # Create heatmap\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(target_corr, annot=True, cmap='RdYlBu_r', center=0,\n",
    "                square=True, linewidths=0.5, fmt='.3f', \n",
    "                cbar_kws={\"shrink\": .8})\n",
    "    plt.title('üîó Target Variable Correlations Across Horizons', fontsize=14, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print correlation insights\n",
    "    print(\"\\nüîç CORRELATION INSIGHTS:\")\n",
    "    for i in range(len(target_cols)):\n",
    "        for j in range(i+1, len(target_cols)):\n",
    "            corr_val = target_corr.iloc[i, j]\n",
    "            col1 = target_cols[i].replace('target_', '').replace('d', '-day')\n",
    "            col2 = target_cols[j].replace('target_', '').replace('d', '-day')\n",
    "            print(f\"  {col1} ‚Üî {col2}: {corr_val:.3f}\")\n",
    "\n",
    "analyze_target_correlations(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c839935a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_cross_sectional_patterns(df):\n",
    "    \"\"\"Analyze patterns across different stocks and sectors\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üîÑ CROSS-SECTIONAL ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    symbols = df['symbol'].unique()\n",
    "    \n",
    "    # 1. Price level comparison\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Plot 1: Current price levels\n",
    "    ax1 = axes[0, 0]\n",
    "    current_prices = []\n",
    "    for symbol in symbols:\n",
    "        current_price = df[df['symbol'] == symbol]['Close'].iloc[-1]\n",
    "        current_prices.append(current_price)\n",
    "    \n",
    "    bars = ax1.bar(symbols, current_prices, color=plt.cm.viridis(np.linspace(0, 1, len(symbols))))\n",
    "    ax1.set_title('üí∞ Current Price Levels', fontsize=14, fontweight='bold')\n",
    "    ax1.set_ylabel('Price ($)')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, price in zip(bars, current_prices):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(current_prices)*0.01,\n",
    "                f'${price:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Plot 2: Volatility comparison\n",
    "    ax2 = axes[0, 1]\n",
    "    avg_volatilities = []\n",
    "    for symbol in symbols:\n",
    "        if 'volatility_20d' in df.columns:\n",
    "            avg_vol = df[df['symbol'] == symbol]['volatility_20d'].mean() * 100\n",
    "        else:\n",
    "            # Calculate simple volatility\n",
    "            returns = df[df['symbol'] == symbol]['Close'].pct_change()\n",
    "            avg_vol = returns.std() * np.sqrt(252) * 100  # Annualized\n",
    "        avg_volatilities.append(avg_vol)\n",
    "    \n",
    "    bars = ax2.bar(symbols, avg_volatilities, color=plt.cm.plasma(np.linspace(0, 1, len(symbols))))\n",
    "    ax2.set_title('üìâ Average Volatility', fontsize=14, fontweight='bold')\n",
    "    ax2.set_ylabel('Volatility (%)')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, vol in zip(bars, avg_volatilities):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(avg_volatilities)*0.01,\n",
    "                f'{vol:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Plot 3: Trading volume comparison\n",
    "    ax3 = axes[1, 0]\n",
    "    avg_volumes = []\n",
    "    for symbol in symbols:\n",
    "        avg_vol = df[df['symbol'] == symbol]['Volume'].mean() / 1_000_000  # In millions\n",
    "        avg_volumes.append(avg_vol)\n",
    "    \n",
    "    bars = ax3.bar(symbols, avg_volumes, color=plt.cm.coolwarm(np.linspace(0, 1, len(symbols))))\n",
    "    ax3.set_title('üìä Average Daily Volume', fontsize=14, fontweight='bold')\n",
    "    ax3.set_ylabel('Volume (Millions)')\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, vol in zip(bars, avg_volumes):\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(avg_volumes)*0.01,\n",
    "                f'{vol:.1f}M', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Plot 4: News coverage comparison\n",
    "    ax4 = axes[1, 1]\n",
    "    if 'news_count' in df.columns:\n",
    "        avg_news = []\n",
    "        for symbol in symbols:\n",
    "            avg_news_count = df[df['symbol'] == symbol]['news_count'].mean()\n",
    "            avg_news.append(avg_news_count)\n",
    "        \n",
    "        bars = ax4.bar(symbols, avg_news, color=plt.cm.spring(np.linspace(0, 1, len(symbols))))\n",
    "        ax4.set_title('üì∞ Average Daily News Coverage', fontsize=14, fontweight='bold')\n",
    "        ax4.set_ylabel('Articles per Day')\n",
    "        ax4.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, news in zip(bars, avg_news):\n",
    "            ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(avg_news)*0.01,\n",
    "                    f'{news:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "    else:\n",
    "        ax4.text(0.5, 0.5, 'No news data\\navailable', ha='center', va='center', \n",
    "                transform=ax4.transAxes, fontsize=12)\n",
    "        ax4.set_title('üì∞ News Coverage (Not Available)', fontsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Cross-sectional statistics table\n",
    "    print(\"\\nüìä CROSS-SECTIONAL STATISTICS SUMMARY:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    stats_data = []\n",
    "    for symbol in symbols:\n",
    "        symbol_data = df[df['symbol'] == symbol]\n",
    "        \n",
    "        # Calculate key metrics\n",
    "        total_return = ((symbol_data['Close'].iloc[-1] / symbol_data['Close'].iloc[0]) - 1) * 100\n",
    "        daily_returns = symbol_data['Close'].pct_change().dropna()\n",
    "        volatility = daily_returns.std() * np.sqrt(252) * 100  # Annualized\n",
    "        sharpe = (daily_returns.mean() * 252) / (daily_returns.std() * np.sqrt(252))\n",
    "        max_drawdown = ((symbol_data['Close'] / symbol_data['Close'].cummax()) - 1).min() * 100\n",
    "        avg_volume = symbol_data['Volume'].mean() / 1_000_000\n",
    "        \n",
    "        news_coverage = symbol_data['news_count'].mean() if 'news_count' in df.columns else 0\n",
    "        \n",
    "        stats_data.append({\n",
    "            'Symbol': symbol,\n",
    "            'Total Return (%)': f\"{total_return:.1f}\",\n",
    "            'Volatility (%)': f\"{volatility:.1f}\",\n",
    "            'Sharpe Ratio': f\"{sharpe:.2f}\",\n",
    "            'Max Drawdown (%)': f\"{max_drawdown:.1f}\",\n",
    "            'Avg Volume (M)': f\"{avg_volume:.1f}\",\n",
    "            'News/Day': f\"{news_coverage:.1f}\"\n",
    "        })\n",
    "    \n",
    "    stats_df = pd.DataFrame(stats_data)\n",
    "    print(stats_df.to_string(index=False))\n",
    "\n",
    "analyze_cross_sectional_patterns(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1e4a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_correlation_analysis(df):\n",
    "    \"\"\"Comprehensive correlation analysis across all feature types\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üìä COMPREHENSIVE CORRELATION ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Exclude non-numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Remove identifier columns\n",
    "    analysis_cols = [col for col in numeric_cols if col not in ['symbol']]\n",
    "    \n",
    "    if len(analysis_cols) < 2:\n",
    "        print(\"‚ö†Ô∏è Not enough numeric features for correlation analysis\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üìä Analyzing correlations for {len(analysis_cols)} features\")\n",
    "    \n",
    "    # Calculate correlation matrix (use subset for visualization)\n",
    "    sample_data = df[analysis_cols].sample(min(10000, len(df)), random_state=42)\n",
    "    corr_matrix = sample_data.corr()\n",
    "    \n",
    "    # 1. Full correlation heatmap (top correlations only)\n",
    "    plt.figure(figsize=(20, 16))\n",
    "    \n",
    "    # Get top correlated features with targets\n",
    "    target_cols = [col for col in analysis_cols if col.startswith('target_')]\n",
    "    if target_cols:\n",
    "        # Find features most correlated with targets\n",
    "        target_corrs = []\n",
    "        for target in target_cols:\n",
    "            for feature in analysis_cols:\n",
    "                if feature != target:\n",
    "                    corr_val = abs(corr_matrix.loc[target, feature])\n",
    "                    target_corrs.append((feature, target, corr_val))\n",
    "        \n",
    "        # Sort by correlation strength\n",
    "        target_corrs.sort(key=lambda x: x[2], reverse=True)\n",
    "        \n",
    "        # Take top 30 features most correlated with any target\n",
    "        top_features = list(set([item[0] for item in target_corrs[:30]]))\n",
    "        top_features.extend(target_cols)\n",
    "        \n",
    "        # Create heatmap for top features\n",
    "        top_corr_matrix = corr_matrix.loc[top_features, top_features]\n",
    "        \n",
    "        sns.heatmap(top_corr_matrix, annot=False, cmap='RdYlBu_r', center=0,\n",
    "                    square=True, linewidths=0.1, cbar_kws={\"shrink\": .8})\n",
    "        plt.title('üîó Feature Correlation Matrix (Top 30 + Targets)', fontsize=16, fontweight='bold', pad=20)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print top correlations with targets\n",
    "        print(\"\\nüéØ TOP FEATURES CORRELATED WITH TARGETS:\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        for target in target_cols:\n",
    "            print(f\"\\n{target}:\")\n",
    "            target_corr_series = corr_matrix[target].abs().sort_values(ascending=False)\n",
    "            # Exclude the target itself\n",
    "            target_corr_series = target_corr_series[target_corr_series.index != target]\n",
    "            \n",
    "            for i, (feature, corr_val) in enumerate(target_corr_series.head(10).items()):\n",
    "                print(f\"  {i+1:2d}. {feature:<25} : {corr_matrix.loc[target, feature]:6.3f}\")\n",
    "    \n",
    "    # 2. Feature group correlation analysis\n",
    "    feature_groups = {\n",
    "        'Price': [col for col in analysis_cols if col in ['Open', 'High', 'Low', 'Close', 'Volume']],\n",
    "        'Technical': [col for col in analysis_cols if any(tech in col for tech in ['SMA', 'EMA', 'RSI', 'MACD', 'BB', 'volatility'])],\n",
    "        'News': [col for col in analysis_cols if 'news' in col.lower()],\n",
    "        'Targets': [col for col in analysis_cols if col.startswith('target_')]\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüìä CORRELATION BY FEATURE GROUPS:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for group_name, group_features in feature_groups.items():\n",
    "        if len(group_features) > 1:\n",
    "            group_corr = corr_matrix.loc[group_features, group_features]\n",
    "            \n",
    "            # Calculate average absolute correlation within group\n",
    "            mask = np.triu(np.ones_like(group_corr, dtype=bool), k=1)\n",
    "            avg_corr = group_corr.where(mask).abs().mean().mean()\n",
    "            \n",
    "            print(f\"\\n{group_name} Features ({len(group_features)} features):\")\n",
    "            print(f\"  Average |correlation|: {avg_corr:.3f}\")\n",
    "            \n",
    "            # Find most correlated pair within group\n",
    "            max_corr = 0\n",
    "            max_pair = None\n",
    "            for i in range(len(group_features)):\n",
    "                for j in range(i+1, len(group_features)):\n",
    "                    corr_val = abs(group_corr.iloc[i, j])\n",
    "                    if corr_val > max_corr:\n",
    "                        max_corr = corr_val\n",
    "                        max_pair = (group_features[i], group_features[j])\n",
    "            \n",
    "            if max_pair:\n",
    "                print(f\"  Highest correlation: {max_pair[0]} ‚Üî {max_pair[1]} ({max_corr:.3f})\")\n",
    "\n",
    "    # 3. Multicollinearity detection\n",
    "    print(f\"\\n‚ö†Ô∏è MULTICOLLINEARITY DETECTION:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    high_corr_pairs = []\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            corr_val = abs(corr_matrix.iloc[i, j])\n",
    "            if corr_val > 0.9:  # Very high correlation threshold\n",
    "                high_corr_pairs.append({\n",
    "                    'Feature 1': corr_matrix.columns[i],\n",
    "                    'Feature 2': corr_matrix.columns[j],\n",
    "                    'Correlation': corr_matrix.iloc[i, j]\n",
    "                })\n",
    "    \n",
    "    if high_corr_pairs:\n",
    "        print(\"üö® HIGH CORRELATION PAIRS (|r| > 0.9) - Potential multicollinearity:\")\n",
    "        for pair in sorted(high_corr_pairs, key=lambda x: abs(x['Correlation']), reverse=True)[:10]:\n",
    "            print(f\"  {pair['Feature 1']:<25} ‚Üî {pair['Feature 2']:<25}: {pair['Correlation']:6.3f}\")\n",
    "    else:\n",
    "        print(\"‚úÖ No severe multicollinearity detected (no |r| > 0.9)\")\n",
    "\n",
    "comprehensive_correlation_analysis(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe7f7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_time_series_patterns(df):\n",
    "    \"\"\"Analyze time series patterns and seasonality\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üìà TIME SERIES PATTERNS ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    symbols = df['symbol'].unique()\n",
    "    \n",
    "    # 1. Seasonality Analysis\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Day of week patterns\n",
    "    ax1 = axes[0, 0]\n",
    "    df_with_dow = df.copy()\n",
    "    df_with_dow['day_of_week'] = df_with_dow.index.day_name()\n",
    "    df_with_dow['daily_return'] = df_with_dow.groupby('symbol')['Close'].pct_change() * 100\n",
    "    \n",
    "    dow_returns = df_with_dow.groupby('day_of_week')['daily_return'].mean()\n",
    "    # Reorder for Monday-Sunday\n",
    "    day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    dow_returns = dow_returns.reindex([day for day in day_order if day in dow_returns.index])\n",
    "    \n",
    "    bars = ax1.bar(range(len(dow_returns)), dow_returns.values, \n",
    "                   color=['red' if x < 0 else 'green' for x in dow_returns.values])\n",
    "    ax1.set_title('üìÖ Day of Week Effect', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Day of Week')\n",
    "    ax1.set_ylabel('Average Return (%)')\n",
    "    ax1.set_xticks(range(len(dow_returns)))\n",
    "    ax1.set_xticklabels([day[:3] for day in dow_returns.index], rotation=45)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.axhline(0, color='black', linestyle='-', alpha=0.7)\n",
    "    \n",
    "    # Month of year patterns\n",
    "    ax2 = axes[0, 1]\n",
    "    df_with_month = df.copy()\n",
    "    df_with_month['month'] = df_with_month.index.month\n",
    "    df_with_month['daily_return'] = df_with_month.groupby('symbol')['Close'].pct_change() * 100\n",
    "    \n",
    "    month_returns = df_with_month.groupby('month')['daily_return'].mean()\n",
    "    month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "                   'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    \n",
    "    bars = ax2.bar(range(len(month_returns)), month_returns.values,\n",
    "                   color=['red' if x < 0 else 'green' for x in month_returns.values])\n",
    "    ax2.set_title('üìÖ Month of Year Effect', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Month')\n",
    "    ax2.set_ylabel('Average Return (%)')\n",
    "    ax2.set_xticks(range(len(month_returns)))\n",
    "    ax2.set_xticklabels([month_names[i-1] for i in month_returns.index], rotation=45)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.axhline(0, color='black', linestyle='-', alpha=0.7)\n",
    "    \n",
    "    # Volatility clustering\n",
    "    ax3 = axes[1, 0]\n",
    "    symbol = symbols[0]  # Use first symbol for detailed analysis\n",
    "    symbol_data = df[df['symbol'] == symbol].copy()\n",
    "    symbol_data['daily_return'] = symbol_data['Close'].pct_change() * 100\n",
    "    symbol_data['abs_return'] = symbol_data['daily_return'].abs()\n",
    "    symbol_data['volatility_ma'] = symbol_data['abs_return'].rolling(20).mean()\n",
    "    \n",
    "    ax3.plot(symbol_data.index, symbol_data['abs_return'], alpha=0.5, color='blue', linewidth=0.5, label='Daily |Return|')\n",
    "    ax3.plot(symbol_data.index, symbol_data['volatility_ma'], color='red', linewidth=2, label='20-day MA')\n",
    "    ax3.set_title(f'üìâ Volatility Clustering - {symbol}', fontsize=14, fontweight='bold')\n",
    "    ax3.set_ylabel('Absolute Return (%)')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Auto-correlation of returns\n",
    "    ax4 = axes[1, 1]\n",
    "    symbol_returns = symbol_data['daily_return'].dropna()\n",
    "    \n",
    "    # Calculate autocorrelations for lags 1-20\n",
    "    lags = range(1, 21)\n",
    "    autocorrs = [symbol_returns.autocorr(lag) for lag in lags]\n",
    "    \n",
    "    ax4.bar(lags, autocorrs, alpha=0.7, color='steelblue')\n",
    "    ax4.set_title(f'üìä Return Autocorrelation - {symbol}', fontsize=14, fontweight='bold')\n",
    "    ax4.set_xlabel('Lag (days)')\n",
    "    ax4.set_ylabel('Autocorrelation')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    ax4.axhline(0, color='black', linestyle='-', alpha=0.7)\n",
    "    \n",
    "    # Add significance bounds (approximate)\n",
    "    n = len(symbol_returns)\n",
    "    bound = 1.96 / np.sqrt(n)\n",
    "    ax4.axhline(bound, color='red', linestyle='--', alpha=0.7, label=f'95% bound (¬±{bound:.3f})')\n",
    "    ax4.axhline(-bound, color='red', linestyle='--', alpha=0.7)\n",
    "    ax4.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print seasonality insights\n",
    "    print(\"\\nüìÖ SEASONALITY INSIGHTS:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    print(f\"üìä Day of Week Effects:\")\n",
    "    for day, return_val in dow_returns.items():\n",
    "        print(f\"  {day}: {return_val:+.3f}%\")\n",
    "    \n",
    "    best_day = dow_returns.idxmax()\n",
    "    worst_day = dow_returns.idxmin()\n",
    "    print(f\"  üéØ Best day: {best_day} ({dow_returns[best_day]:+.3f}%)\")\n",
    "    print(f\"  üìâ Worst day: {worst_day} ({dow_returns[worst_day]:+.3f}%)\")\n",
    "    \n",
    "    print(f\"\\nüìä Month of Year Effects:\")\n",
    "    for month_num, return_val in month_returns.items():\n",
    "        month_name = month_names[month_num-1]\n",
    "        print(f\"  {month_name}: {return_val:+.3f}%\")\n",
    "    \n",
    "    best_month = month_returns.idxmax()\n",
    "    worst_month = month_returns.idxmin()\n",
    "    print(f\"  üéØ Best month: {month_names[best_month-1]} ({month_returns[best_month]:+.3f}%)\")\n",
    "    print(f\"  üìâ Worst month: {month_names[worst_month-1]} ({month_returns[worst_month]:+.3f}%)\")\n",
    "\n",
    "analyze_time_series_patterns(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f1ad58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_insights_and_recommendations(df):\n",
    "    \"\"\"Generate key insights and recommendations based on the analysis\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üí° KEY INSIGHTS & RECOMMENDATIONS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    symbols = df['symbol'].unique()\n",
    "    \n",
    "    insights = []\n",
    "    recommendations = []\n",
    "    \n",
    "    # 1. Data Quality Insights\n",
    "    total_missing = df.isnull().sum().sum()\n",
    "    total_cells = df.shape[0] * df.shape[1]\n",
    "    missing_pct = (total_missing / total_cells) * 100\n",
    "    \n",
    "    if missing_pct < 1:\n",
    "        insights.append(f\"‚úÖ Excellent data quality: Only {missing_pct:.2f}% missing values\")\n",
    "    elif missing_pct < 5:\n",
    "        insights.append(f\"‚ö†Ô∏è Good data quality: {missing_pct:.2f}% missing values\")\n",
    "        recommendations.append(\"Consider imputation strategies for missing values\")\n",
    "    else:\n",
    "        insights.append(f\"üö® Data quality concerns: {missing_pct:.2f}% missing values\")\n",
    "        recommendations.append(\"Prioritize data cleaning and missing value handling\")\n",
    "    \n",
    "    # 2. Target Variable Insights\n",
    "    target_cols = [col for col in df.columns if col.startswith('target_')]\n",
    "    if target_cols:\n",
    "        for target_col in target_cols:\n",
    "            returns = df[target_col].dropna() * 100\n",
    "            horizon = target_col.replace('target_', '').replace('d', '')\n",
    "            \n",
    "            sharpe = returns.mean() / returns.std() if returns.std() > 0 else 0\n",
    "            positive_pct = (returns > 0).mean() * 100\n",
    "            \n",
    "            insights.append(f\"üìä {horizon}-day forecasting: {positive_pct:.1f}% positive returns, Sharpe: {sharpe:.3f}\")\n",
    "            \n",
    "            if positive_pct > 55:\n",
    "                insights.append(f\"üéØ {horizon}-day horizon shows positive bias - good for long strategies\")\n",
    "            elif positive_pct < 45:\n",
    "                insights.append(f\"üìâ {horizon}-day horizon shows negative bias - consider short strategies\")\n",
    "    \n",
    "    # 3. Volatility Insights\n",
    "    if 'volatility_20d' in df.columns:\n",
    "        avg_vol = df['volatility_20d'].mean() * 100\n",
    "        insights.append(f\"üìâ Average portfolio volatility: {avg_vol:.1f}% (20-day)\")\n",
    "        \n",
    "        if avg_vol > 30:\n",
    "            recommendations.append(\"High volatility detected - consider volatility-adjusted position sizing\")\n",
    "        elif avg_vol < 15:\n",
    "            insights.append(\"üìä Low volatility environment - suitable for higher leverage strategies\")\n",
    "    \n",
    "    # 4. News Coverage Insights\n",
    "    if 'news_count' in df.columns:\n",
    "        avg_news = df['news_count'].mean()\n",
    "        insights.append(f\"üì∞ Average news coverage: {avg_news:.1f} articles/day/symbol\")\n",
    "        \n",
    "        if avg_news > 5:\n",
    "            insights.append(\"üìà Rich news environment - sentiment analysis highly valuable\")\n",
    "            recommendations.append(\"Implement advanced NLP techniques for sentiment extraction\")\n",
    "        elif avg_news < 1:\n",
    "            insights.append(\"üì∞ Limited news coverage - consider expanding news sources\")\n",
    "            recommendations.append(\"Add more news sources or consider alternative text data\")\n",
    "    \n",
    "    # 5. Cross-Sectional Insights\n",
    "    total_return_by_symbol = {}\n",
    "    vol_by_symbol = {}\n",
    "    \n",
    "    for symbol in symbols:\n",
    "        symbol_data = df[df['symbol'] == symbol]\n",
    "        total_return = ((symbol_data['Close'].iloc[-1] / symbol_data['Close'].iloc[0]) - 1) * 100\n",
    "        daily_returns = symbol_data['Close'].pct_change().dropna()\n",
    "        volatility = daily_returns.std() * np.sqrt(252) * 100\n",
    "        \n",
    "        total_return_by_symbol[symbol] = total_return\n",
    "        vol_by_symbol[symbol] = volatility\n",
    "    \n",
    "    best_performer = max(total_return_by_symbol, key=total_return_by_symbol.get)\n",
    "    worst_performer = min(total_return_by_symbol, key=total_return_by_symbol.get)\n",
    "    \n",
    "    insights.append(f\"üèÜ Best performer: {best_performer} ({total_return_by_symbol[best_performer]:+.1f}%)\")\n",
    "    insights.append(f\"üìâ Worst performer: {worst_performer} ({total_return_by_symbol[worst_performer]:+.1f}%)\")\n",
    "    \n",
    "    # 6. Feature Engineering Insights\n",
    "    feature_groups = {\n",
    "        'Technical': [col for col in df.columns if any(tech in col for tech in ['SMA', 'EMA', 'RSI', 'MACD', 'BB'])],\n",
    "        'News': [col for col in df.columns if 'news' in col.lower()],\n",
    "        'Lag': [col for col in df.columns if 'lag' in col.lower()]\n",
    "    }\n",
    "    \n",
    "    for group, features in feature_groups.items():\n",
    "        if features:\n",
    "            insights.append(f\"üîß {group} features: {len(features)} engineered\")\n",
    "        else:\n",
    "            recommendations.append(f\"Consider adding {group.lower()} features\")\n",
    "    \n",
    "    # Print insights\n",
    "    print(\"\\nüîç KEY INSIGHTS:\")\n",
    "    for i, insight in enumerate(insights, 1):\n",
    "        print(f\"{i:2d}. {insight}\")\n",
    "    \n",
    "    print(\"\\nüìã RECOMMENDATIONS:\")\n",
    "    for i, rec in enumerate(recommendations, 1):\n",
    "        print(f\"{i:2d}. {rec}\")\n",
    "    \n",
    "    # 7. Model Training Recommendations\n",
    "    print(\"\\nü§ñ MODEL TRAINING RECOMMENDATIONS:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    model_recs = [\n",
    "        \"Use cross-validation with time series splits (avoid look-ahead bias)\",\n",
    "        \"Implement early stopping to prevent overfitting\",\n",
    "        \"Consider ensemble methods to improve robustness\",\n",
    "        \"Use proper evaluation metrics: Sharpe ratio, directional accuracy, risk-adjusted returns\",\n",
    "        \"Implement walk-forward analysis for realistic backtesting\",\n",
    "    ]\n",
    "    \n",
    "    # Add specific recommendations based on data characteristics\n",
    "    if len(symbols) > 1:\n",
    "        model_recs.append(\"Leverage cross-sectional information in multi-stock models\")\n",
    "    \n",
    "    if 'news_count' in df.columns and df['news_count'].mean() > 2:\n",
    "        model_recs.append(\"Implement temporal decay for sentiment features (key innovation)\")\n",
    "    \n",
    "    if len([col for col in df.columns if 'volatility' in col]) > 0:\n",
    "        model_recs.append(\"Use volatility features for dynamic position sizing\")\n",
    "    \n",
    "    for i, rec in enumerate(model_recs, 1):\n",
    "        print(f\"{i:2d}. {rec}\")\n",
    "    \n",
    "    print(\"\\nüéØ NEXT STEPS:\")\n",
    "    print(\"=\" * 30)\n",
    "    print(\"1. Run Step 2: Sentiment Analysis\")\n",
    "    print(\"   ‚Üí python run_experiment.py --step 2\")\n",
    "    print(\"2. Run Step 3: Temporal Decay Processing\")\n",
    "    print(\"   ‚Üí python run_experiment.py --step 3\")\n",
    "    print(\"3. Run Complete Pipeline\")\n",
    "    print(\"   ‚Üí python run_experiment.py\")\n",
    "\n",
    "generate_insights_and_recommendations(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dc9e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary_dashboard(df):\n",
    "    \"\"\"Create a comprehensive summary dashboard\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üìä DATASET SUMMARY DASHBOARD\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Key metrics\n",
    "    symbols = df['symbol'].unique()\n",
    "    date_range = (df.index.max() - df.index.min()).days\n",
    "    \n",
    "    print(f\"üìÖ Analysis Period: {df.index.min().date()} to {df.index.max().date()} ({date_range} days)\")\n",
    "    print(f\"üìà Symbols Analyzed: {len(symbols)} ({', '.join(symbols)})\")\n",
    "    print(f\"üìä Dataset Size: {df.shape[0]:,} rows √ó {df.shape[1]} features\")\n",
    "    print(f\"üíæ Memory Usage: {df.memory_usage(deep=True).sum() / 1024 / 1024:.1f} MB\")\n",
    "    \n",
    "    # Feature breakdown\n",
    "    feature_counts = {\n",
    "        'Price': len([col for col in df.columns if col in ['Open', 'High', 'Low', 'Close', 'Volume']]),\n",
    "        'Technical': len([col for col in df.columns if any(tech in col for tech in ['SMA', 'EMA', 'RSI', 'MACD', 'BB', 'volatility'])]),\n",
    "        'News': len([col for col in df.columns if 'news' in col.lower()]),\n",
    "        'Targets': len([col for col in df.columns if col.startswith('target_')]),\n",
    "        'Other': df.shape[1] - sum([\n",
    "            len([col for col in df.columns if col in ['Open', 'High', 'Low', 'Close', 'Volume']]),\n",
    "            len([col for col in df.columns if any(tech in col for tech in ['SMA', 'EMA', 'RSI', 'MACD', 'BB', 'volatility'])]),\n",
    "            len([col for col in df.columns if 'news' in col.lower()]),\n",
    "            len([col for col in df.columns if col.startswith('target_')])\n",
    "        ])\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüîß Feature Breakdown:\")\n",
    "    for feature_type, count in feature_counts.items():\n",
    "        if count > 0:\n",
    "            print(f\"   {feature_type}: {count} features\")\n",
    "    \n",
    "    # Data quality summary\n",
    "    missing_pct = (df.isnull().sum().sum() / (df.shape[0] * df.shape[1])) * 100\n",
    "    print(f\"\\nüîç Data Quality:\")\n",
    "    print(f\"   Missing Values: {missing_pct:.2f}%\")\n",
    "    print(f\"   Complete Cases: {df.dropna().shape[0]:,} ({(df.dropna().shape[0]/df.shape[0])*100:.1f}%)\")\n",
    "    \n",
    "    # Performance summary\n",
    "    print(f\"\\nüìà Performance Summary (Total Returns):\")\n",
    "    for symbol in symbols:\n",
    "        symbol_data = df[df['symbol'] == symbol]\n",
    "        total_return = ((symbol_data['Close'].iloc[-1] / symbol_data['Close'].iloc[0]) - 1) * 100\n",
    "        volatility = symbol_data['Close'].pct_change().std() * np.sqrt(252) * 100\n",
    "        print(f\"   {symbol}: {total_return:+6.1f}% (Vol: {volatility:4.1f}%)\")\n",
    "    \n",
    "    # News coverage summary\n",
    "    if 'news_count' in df.columns:\n",
    "        print(f\"\\nüì∞ News Coverage:\")\n",
    "        total_articles = df['news_count'].sum()\n",
    "        avg_daily = df['news_count'].mean()\n",
    "        print(f\"   Total Articles: {total_articles:,.0f}\")\n",
    "        print(f\"   Average/Day: {avg_daily:.1f}\")\n",
    "        \n",
    "        for symbol in symbols:\n",
    "            symbol_news = df[df['symbol'] == symbol]['news_count'].mean()\n",
    "            print(f\"   {symbol}: {symbol_news:.1f} articles/day\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Dataset ready for sentiment analysis and temporal decay processing!\")\n",
    "    print(f\"üöÄ Run: python run_experiment.py --step 2\")\n",
    "\n",
    "create_summary_dashboard(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
