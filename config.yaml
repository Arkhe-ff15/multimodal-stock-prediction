# ===================================================================
# COMPREHENSIVE SENTIMENT-TFT PIPELINE CONFIGURATION
# Single source of truth for data, model, training, and evaluation
# ===================================================================

# =================
# PIPELINE CONTROL
# =================
pipeline:
  # Which stages to run (set to false to skip)
  stages:
    data_processing: true
    fnspid_processing: true 
    temporal_decay: true
    sentiment_analysis: true
    feature_engineering: true
    model_training: true
    model_evaluation: true
    model_inference: true
  
  # Pipeline execution settings
  execution:
    parallel_processing: true
    max_workers: 4
    memory_limit_gb: 16
    checkpoint_enabled: true
    resume_from_checkpoint: true

# ===============
# DATA SETTINGS
# ===============
data:
  # Core dataset configuration (data.py)
  core:
    symbols: ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'NVDA', 'TSLA', 'META', 'NFLX']
    start_date: '2018-12-01'
    end_date: '2024-01-31'
    target_horizons: [5, 10, 30, 60, 90]
    
    # Data sources
    yahoo_finance:
      enabled: true
      features: ['Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']
      
    alpha_vantage:
      enabled: false
      api_key: null  # Set your API key here
      
  # FNSPID processing configuration (fnspid_processor.py)
  fnspid:
    # Production settings
    production:
      sample_ratio: 0.15  # 15% of 22GB dataset for production
      chunk_size: 75000
      max_articles_per_day: 100
      min_article_length: 50
      max_article_length: 2000
      
    # Development/testing settings  
    development:
      sample_ratio: 0.03  # 3% for quick testing
      chunk_size: 10000
      max_articles_per_day: 20
      symbols_subset: ['AAPL', 'MSFT']
      date_range: ['2023-01-01', '2023-06-30']
      
    # Processing parameters
    processing:
      remove_duplicates: true
      filter_weekends: true
      min_confidence_score: 0.6
      language_filter: 'english'
      
  # Temporal decay configuration (temporal_decay.py)
  temporal_decay:
    # Decay function parameters
    decay_function: 'exponential'  # Options: exponential, linear, gaussian
    decay_rate: 0.1
    max_decay_days: 30
    
    # Feature engineering
    features:
      sentiment_momentum: true
      sentiment_volatility: true
      sentiment_trend: true
      rolling_windows: [3, 7, 14, 30]
      
    # Memory optimization
    batch_processing: true
    batch_size: 10000

# =================
# SENTIMENT ANALYSIS
# =================
sentiment:
  # Model configuration (sentiment.py)
  models:
    finbert:
      model_name: 'ProsusAI/finbert'
      enabled: true
      batch_size: 16
      max_length: 512
      device: 'cpu'  # Options: cpu, cuda, mps
      
    vader:
      enabled: false  # Backup sentiment analyzer
      
    custom_bert:
      enabled: false
      model_path: null
      
  # Processing settings
  processing:
    aggregate_daily: true
    weight_by_confidence: true
    normalize_scores: true
    handle_missing: 'interpolate'  # Options: drop, interpolate, forward_fill

# ================
# MODEL SETTINGS
# ================
model:
  # TFT Architecture
  tft:
    # Model architecture
    hidden_size: 128
    attention_head_size: 4
    dropout: 0.2
    hidden_continuous_size: 8
    
    # Input configuration
    static_categoricals: []
    static_reals: []
    time_varying_known_categoricals: []
    time_varying_known_reals: ['day_of_week', 'month', 'holiday']
    time_varying_unknown_categoricals: []
    time_varying_unknown_reals: [
      'close_price', 'volume', 'volatility',
      'sentiment_score', 'sentiment_confidence',
      'sentiment_momentum', 'sentiment_volatility'
    ]
    
    # Sequence configuration
    max_encoder_length: 60  # Days of historical data
    max_prediction_length: 30  # Days to predict
    
    # Target configuration
    target: 'price_return'
    target_normalizer: 'GroupNormalizer'
    
  # Alternative models (for comparison)
  baselines:
    lstm:
      enabled: true
      hidden_size: 64
      num_layers: 2
      
    transformer:
      enabled: false
      d_model: 128
      nhead: 8
      
    linear:
      enabled: true  # Simple linear baseline

# =================
# TRAINING SETTINGS
# =================
training:
  # General training configuration
  general:
    max_epochs: 100
    early_stopping_patience: 10
    learning_rate: 0.001
    weight_decay: 1e-5
    batch_size: 64
    validation_split: 0.2
    test_split: 0.1
    
    # Learning rate scheduling
    lr_scheduler:
      type: 'ReduceLROnPlateau'  # Options: ReduceLROnPlateau, StepLR, CosineAnnealingLR
      factor: 0.5
      patience: 5
      
  # Optimization
  optimization:
    optimizer: 'Adam'  # Options: Adam, AdamW, SGD
    gradient_clipping: 1.0
    mixed_precision: false
    
  # Data loading
  data_loading:
    num_workers: 4
    pin_memory: true
    prefetch_factor: 2
    
  # Regularization
  regularization:
    dropout_rate: 0.2
    l2_regularization: 1e-5
    label_smoothing: 0.0

# ==================
# EVALUATION SETTINGS
# ==================
evaluation:
  # Metrics to compute
  metrics:
    regression:
      - 'mae'  # Mean Absolute Error
      - 'mse'  # Mean Squared Error
      - 'rmse'  # Root Mean Squared Error
      - 'mape'  # Mean Absolute Percentage Error
      - 'r2'   # R-squared
      
    trading:
      - 'sharpe_ratio'
      - 'max_drawdown'
      - 'annualized_return'
      - 'win_rate'
      - 'profit_factor'
      
    statistical:
      - 'directional_accuracy'
      - 'hit_rate'
      - 'information_ratio'
      
  # Evaluation configuration
  settings:
    cross_validation:
      enabled: true
      n_folds: 5
      time_series_split: true
      
    backtesting:
      enabled: true
      start_date: '2023-01-01'
      end_date: '2024-01-31'
      rebalance_frequency: 'monthly'
      
    statistical_tests:
      significance_level: 0.05
      bootstrap_samples: 1000

# ===============
# FILE PATHS
# ===============
paths:
  # Base directories
  base_dir: '.'
  data_dir: 'data'
  models_dir: 'models'
  results_dir: 'results'
  logs_dir: 'logs'
  
  # Raw data files
  raw:
    fnspid_data: 'data/raw/nasdaq_exteral_data.csv'
    yahoo_data: 'data/raw/yahoo_finance_data.csv'
    
  # Processed data files
  processed:
    core_dataset: 'data/processed/combined_dataset.csv'
    fnspid_filtered_articles: 'data/processed/fnspid_filtered_articles.csv'
    fnspid_article_sentiment: 'data/processed/fnspid_article_sentiment.csv'
    fnspid_daily_sentiment: 'data/processed/fnspid_daily_sentiment.csv'
    temporal_decay_dataset: 'data/processed/temporal_decay_enhanced_dataset.csv'
    final_dataset: 'data/processed/final_dataset.csv'
    
  # Model files
  models:
    tft_checkpoint: 'models/tft/best_model.ckpt'
    tft_final: 'models/tft/final_model.pkl'
    lstm_model: 'models/baselines/lstm_model.pkl'
    linear_model: 'models/baselines/linear_model.pkl'
    
  # Results files
  results:
    training_logs: 'results/training_logs.csv'
    evaluation_results: 'results/evaluation_results.json'
    predictions: 'results/predictions.csv'
    backtest_results: 'results/backtest_results.csv'
    feature_importance: 'results/feature_importance.csv'

# =================
# LOGGING & OUTPUT
# =================
logging:
  # General logging
  level: 'INFO'  # Options: DEBUG, INFO, WARNING, ERROR
  format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
  
  # File logging
  file_logging:
    enabled: true
    log_file: 'logs/pipeline.log'
    max_size_mb: 50
    backup_count: 5
    
  # Console logging
  console_logging:
    enabled: true
    colored: true
    
  # Progress tracking
  progress:
    show_progress_bars: true
    log_every_n_steps: 100
    
  # MLflow tracking (optional)
  mlflow:
    enabled: false
    tracking_uri: null
    experiment_name: 'sentiment_tft'

# =================
# SYSTEM SETTINGS
# =================
system:
  # Hardware configuration
  hardware:
    device: 'auto'  # Options: auto, cpu, cuda, mps
    gpu_memory_fraction: 0.8
    cpu_threads: null  # null = auto-detect
    
  # Memory management
  memory:
    max_memory_usage_gb: 16
    garbage_collection: true
    memory_profiling: false
    
  # Reproducibility
  reproducibility:
    random_seed: 42
    deterministic: true
    benchmark: false
    
  # Performance
  performance:
    compile_model: false  # PyTorch 2.0+ compilation
    use_mixed_precision: false
    gradient_checkpointing: false

# ==================
# FEATURE ENGINEERING
# ==================
features:
  # Price-based features
  price_features:
    returns: true
    log_returns: true
    volatility: true
    rolling_statistics: [5, 10, 20, 50]
    
  # Technical indicators
  technical:
    sma: [10, 20, 50]  # Simple Moving Average periods
    ema: [12, 26]      # Exponential Moving Average periods
    rsi: 14            # RSI period
    macd: true
    bollinger_bands: true
    
  # Sentiment features
  sentiment_features:
    raw_sentiment: true
    rolling_sentiment: [3, 7, 14]
    sentiment_momentum: true
    sentiment_divergence: true
    
  # Market regime features
  market_regime:
    vix_level: true
    market_stress: true
    sector_rotation: true
    
  # Calendar features
  calendar:
    day_of_week: true
    month: true
    quarter: true
    holidays: true
    earnings_season: true

# ==================
# HYPERPARAMETER TUNING
# ==================
hyperparameter_tuning:
  # General settings
  enabled: false
  method: 'optuna'  # Options: optuna, ray_tune, grid_search
  n_trials: 50
  timeout_hours: 24
  
  # Search space
  search_space:
    learning_rate: [0.0001, 0.01]
    hidden_size: [64, 128, 256]
    dropout: [0.1, 0.5]
    batch_size: [32, 64, 128]
    
  # Optimization
  objective: 'val_loss'
  direction: 'minimize'
  pruning: true

# =================
# DEPLOYMENT
# =================
deployment:
  # Model serving
  serving:
    enabled: false
    framework: 'fastapi'  # Options: fastapi, flask, torchserve
    host: '0.0.0.0'
    port: 8000
    
  # Monitoring
  monitoring:
    model_drift: true
    data_drift: true
    performance_monitoring: true
    
  # Batch inference
  batch_inference:
    schedule: 'daily'
    output_format: 'csv'
    notification: false
